{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "414b5fc1",
   "metadata": {
    "papermill": {
     "duration": 0.191548,
     "end_time": "2022-06-19T05:46:03.562074",
     "exception": false,
     "start_time": "2022-06-19T05:46:03.370526",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Ensemble of the three notebooks.Please upvote the original notebooks:\n",
    "\n",
    "**1. Deberta v3 large_transformer_head**\n",
    "[Deberta_V3_Large_Transformer_Head](https://www.kaggle.com/code/jujutsuns/deberta-v3-large-transformer-head?scriptVersionId=95340789)\n",
    "\n",
    "**2. anferico/bert-for-patents**\n",
    "[patent_phrase_matching [inference]](https://www.kaggle.com/code/barangizagiza/patent-phrase-matching-inference?scriptVersionId=98350930)\n",
    "\n",
    "**3. anferico/bert-for-patents**\n",
    "[tez inference phrase matching base](https://www.kaggle.com/code/jujutsuns/tez-inference-phrase-matching-skf?scriptVersionId=97967285)\n",
    "\n",
    "**4. RoBERTa-large**\n",
    "[PatentPhrase RoBERTa Inference](https://www.kaggle.com/code/santhoshkumarv/patentphrase-roberta-inference-lb-0-814)\n",
    "\n",
    "**5. Deberta v3 large**\n",
    "[Deberta v3 large + additional fold(s)](https://www.kaggle.com/code/renokan/pppm-deberta-v3-large-additional-fold-s/notebook?scriptVersionId=96750400)\n",
    "\n",
    "**6. Deberta v3 large_transformer_head**\n",
    "[Deberta_V3_Large_Transformer_Head](https://www.kaggle.com/code/jujutsuns/deberta-v3-large-transformer-head?scriptVersionId=96208185)\n",
    "\n",
    "**7. Electra**\n",
    "[Electra](https://www.kaggle.com/code/gaozhao/a-simple-ensemble-of-two/notebook)\n",
    "\n",
    "**8. Electra**\n",
    "[Electra](https://www.kaggle.com/code/jujutsuns/electra?scriptVersionId=98672999)\n",
    "\n",
    "**9. xlm roberta large**\n",
    "[xlm roberta large [inference]](https://www.kaggle.com/code/jujutsuns/xlm-roberta-large-inference?scriptVersionId=98231498)\n",
    "\n",
    "And use ensemble strategy from:\n",
    "[Tips for ensambling](https://www.kaggle.com/code/jellyz9/tips-for-ensambling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481e6f44",
   "metadata": {
    "papermill": {
     "duration": 0.166749,
     "end_time": "2022-06-19T05:46:03.905555",
     "exception": false,
     "start_time": "2022-06-19T05:46:03.738806",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1.Deberta v3 large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efccb1ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:46:04.195484Z",
     "iopub.status.busy": "2022-06-19T05:46:04.195088Z",
     "iopub.status.idle": "2022-06-19T05:46:04.210982Z",
     "shell.execute_reply": "2022-06-19T05:46:04.210136Z"
    },
    "papermill": {
     "duration": 0.137055,
     "end_time": "2022-06-19T05:46:04.213967",
     "exception": false,
     "start_time": "2022-06-19T05:46:04.076912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "INPUT_DIR = '../input/us-patent-phrase-to-phrase-matching/'\n",
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fbdc189",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:46:04.438586Z",
     "iopub.status.busy": "2022-06-19T05:46:04.437860Z",
     "iopub.status.idle": "2022-06-19T05:46:04.444217Z",
     "shell.execute_reply": "2022-06-19T05:46:04.443145Z"
    },
    "papermill": {
     "duration": 0.121283,
     "end_time": "2022-06-19T05:46:04.446351",
     "exception": false,
     "start_time": "2022-06-19T05:46:04.325068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    num_workers=4\n",
    "    path=\"../input/pppm-deberta-v3-large-baseline-w-w-b-train/\"\n",
    "    config_path=path+'config.pth'\n",
    "    model=\"microsoft/deberta-v3-large\"\n",
    "    batch_size=32\n",
    "    fc_dropout=0.2\n",
    "    target_size=1\n",
    "    max_len=133\n",
    "    seed=42\n",
    "    n_fold=4\n",
    "    trn_fold=[0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48ca8537",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:46:04.674292Z",
     "iopub.status.busy": "2022-06-19T05:46:04.673751Z",
     "iopub.status.idle": "2022-06-19T05:46:43.780620Z",
     "shell.execute_reply": "2022-06-19T05:46:43.779406Z"
    },
    "papermill": {
     "duration": 39.223102,
     "end_time": "2022-06-19T05:46:43.782782",
     "exception": false,
     "start_time": "2022-06-19T05:46:04.559680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.18.0\n",
      "Uninstalling transformers-4.18.0:\n",
      "  Successfully uninstalled transformers-4.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tokenizers 0.12.1\n",
      "Uninstalling tokenizers-0.12.1:\n",
      "  Successfully uninstalled tokenizers-0.12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: ../input/pppm-pip-wheels-dataset\n",
      "Processing /kaggle/input/pppm-pip-wheels-dataset/transformers-4.16.2-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Processing /kaggle/input/pppm-pip-wheels-dataset/tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "Successfully installed tokenizers-0.11.6 transformers-4.16.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: ../input/pppm-pip-wheels-dataset\n",
      "Requirement already satisfied: tokenizers in /opt/conda/lib/python3.7/site-packages (0.11.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers.__version__: 0.11.6\n",
      "transformers.__version__: 4.16.2\n",
      "tokenizers.__version__: 0.11.6\n",
      "transformers.__version__: 4.16.2\n",
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "os.system('pip uninstall -y transformers')\n",
    "os.system('pip uninstall -y tokenizers')\n",
    "os.system('python -m pip install --no-index --find-links=../input/pppm-pip-wheels-dataset transformers')\n",
    "os.system('python -m pip install --no-index --find-links=../input/pppm-pip-wheels-dataset tokenizers')\n",
    "import tokenizers\n",
    "import transformers\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "\n",
    "from transformers import AdamW, AutoConfig, AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, AutoModelForTokenClassification, AutoModelForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from transformers import TrainingArguments, Trainer\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b04c884",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:46:44.002019Z",
     "iopub.status.busy": "2022-06-19T05:46:44.001771Z",
     "iopub.status.idle": "2022-06-19T05:46:44.014757Z",
     "shell.execute_reply": "2022-06-19T05:46:44.013894Z"
    },
    "papermill": {
     "duration": 0.124176,
     "end_time": "2022-06-19T05:46:44.016878",
     "exception": false,
     "start_time": "2022-06-19T05:46:43.892702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_true, y_pred):\n",
    "    score = sp.stats.pearsonr(y_true, y_pred)[0]\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5ad6af4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:46:44.234366Z",
     "iopub.status.busy": "2022-06-19T05:46:44.233618Z",
     "iopub.status.idle": "2022-06-19T05:46:44.247643Z",
     "shell.execute_reply": "2022-06-19T05:46:44.246771Z"
    },
    "papermill": {
     "duration": 0.125322,
     "end_time": "2022-06-19T05:46:44.250124",
     "exception": false,
     "start_time": "2022-06-19T05:46:44.124802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "test = pd.read_csv(INPUT_DIR+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3f4be0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:46:44.469364Z",
     "iopub.status.busy": "2022-06-19T05:46:44.468537Z",
     "iopub.status.idle": "2022-06-19T05:46:44.507718Z",
     "shell.execute_reply": "2022-06-19T05:46:44.506861Z"
    },
    "papermill": {
     "duration": 0.151761,
     "end_time": "2022-06-19T05:46:44.509625",
     "exception": false,
     "start_time": "2022-06-19T05:46:44.357864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>context_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>opc drum</td>\n",
       "      <td>inorganic photoconductor drum</td>\n",
       "      <td>G02</td>\n",
       "      <td>PHYSICS. OPTICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09e418c93a776564</td>\n",
       "      <td>adjust gas flow</td>\n",
       "      <td>altering gas flow</td>\n",
       "      <td>F23</td>\n",
       "      <td>MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36baf228038e314b</td>\n",
       "      <td>lower trunnion</td>\n",
       "      <td>lower locating</td>\n",
       "      <td>B60</td>\n",
       "      <td>PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f37ead645e7f0c8</td>\n",
       "      <td>cap component</td>\n",
       "      <td>upper portion</td>\n",
       "      <td>D06</td>\n",
       "      <td>TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71a5b6ad068d531f</td>\n",
       "      <td>neural stimulation</td>\n",
       "      <td>artificial neural network</td>\n",
       "      <td>H04</td>\n",
       "      <td>ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id              anchor                         target  \\\n",
       "0  4112d61851461f60            opc drum  inorganic photoconductor drum   \n",
       "1  09e418c93a776564     adjust gas flow              altering gas flow   \n",
       "2  36baf228038e314b      lower trunnion                 lower locating   \n",
       "3  1f37ead645e7f0c8       cap component                  upper portion   \n",
       "4  71a5b6ad068d531f  neural stimulation      artificial neural network   \n",
       "\n",
       "  context                                       context_text  \n",
       "0     G02                                    PHYSICS. OPTICS  \n",
       "1     F23  MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...  \n",
       "2     B60  PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...  \n",
       "3     D06  TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...  \n",
       "4     H04      ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# CPC Data\n",
    "# ====================================================\n",
    "cpc_texts = torch.load(CFG.path+\"cpc_texts.pth\")\n",
    "test['context_text'] = test['context'].map(cpc_texts)\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3de43d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:46:44.726798Z",
     "iopub.status.busy": "2022-06-19T05:46:44.726162Z",
     "iopub.status.idle": "2022-06-19T05:46:44.741685Z",
     "shell.execute_reply": "2022-06-19T05:46:44.740651Z"
    },
    "papermill": {
     "duration": 0.125844,
     "end_time": "2022-06-19T05:46:44.743874",
     "exception": false,
     "start_time": "2022-06-19T05:46:44.618030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>context_text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>opc drum</td>\n",
       "      <td>inorganic photoconductor drum</td>\n",
       "      <td>G02</td>\n",
       "      <td>PHYSICS. OPTICS</td>\n",
       "      <td>opc drum[SEP]inorganic photoconductor drum[SEP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09e418c93a776564</td>\n",
       "      <td>adjust gas flow</td>\n",
       "      <td>altering gas flow</td>\n",
       "      <td>F23</td>\n",
       "      <td>MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...</td>\n",
       "      <td>adjust gas flow[SEP]altering gas flow[SEP]MECH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36baf228038e314b</td>\n",
       "      <td>lower trunnion</td>\n",
       "      <td>lower locating</td>\n",
       "      <td>B60</td>\n",
       "      <td>PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...</td>\n",
       "      <td>lower trunnion[SEP]lower locating[SEP]PERFORMI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f37ead645e7f0c8</td>\n",
       "      <td>cap component</td>\n",
       "      <td>upper portion</td>\n",
       "      <td>D06</td>\n",
       "      <td>TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...</td>\n",
       "      <td>cap component[SEP]upper portion[SEP]TEXTILES; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71a5b6ad068d531f</td>\n",
       "      <td>neural stimulation</td>\n",
       "      <td>artificial neural network</td>\n",
       "      <td>H04</td>\n",
       "      <td>ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE</td>\n",
       "      <td>neural stimulation[SEP]artificial neural netwo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id              anchor                         target  \\\n",
       "0  4112d61851461f60            opc drum  inorganic photoconductor drum   \n",
       "1  09e418c93a776564     adjust gas flow              altering gas flow   \n",
       "2  36baf228038e314b      lower trunnion                 lower locating   \n",
       "3  1f37ead645e7f0c8       cap component                  upper portion   \n",
       "4  71a5b6ad068d531f  neural stimulation      artificial neural network   \n",
       "\n",
       "  context                                       context_text  \\\n",
       "0     G02                                    PHYSICS. OPTICS   \n",
       "1     F23  MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...   \n",
       "2     B60  PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...   \n",
       "3     D06  TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...   \n",
       "4     H04      ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE   \n",
       "\n",
       "                                                text  \n",
       "0  opc drum[SEP]inorganic photoconductor drum[SEP...  \n",
       "1  adjust gas flow[SEP]altering gas flow[SEP]MECH...  \n",
       "2  lower trunnion[SEP]lower locating[SEP]PERFORMI...  \n",
       "3  cap component[SEP]upper portion[SEP]TEXTILES; ...  \n",
       "4  neural stimulation[SEP]artificial neural netwo...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1077b7b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:46:44.964514Z",
     "iopub.status.busy": "2022-06-19T05:46:44.963654Z",
     "iopub.status.idle": "2022-06-19T05:46:45.802086Z",
     "shell.execute_reply": "2022-06-19T05:46:45.798436Z"
    },
    "papermill": {
     "duration": 0.952635,
     "end_time": "2022-06-19T05:46:45.806600",
     "exception": false,
     "start_time": "2022-06-19T05:46:44.853965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# tokenizer\n",
    "# ====================================================\n",
    "CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84a7577f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:46:46.050898Z",
     "iopub.status.busy": "2022-06-19T05:46:46.050617Z",
     "iopub.status.idle": "2022-06-19T05:46:46.059485Z",
     "shell.execute_reply": "2022-06-19T05:46:46.058527Z"
    },
    "papermill": {
     "duration": 0.123383,
     "end_time": "2022-06-19T05:46:46.061791",
     "exception": false,
     "start_time": "2022-06-19T05:46:45.938408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(cfg, text):\n",
    "    inputs = cfg.tokenizer(text,\n",
    "                           add_special_tokens=True,\n",
    "                           max_length=cfg.max_len,\n",
    "                           padding=\"max_length\",\n",
    "                           return_offsets_mapping=False)\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['text'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, self.texts[item])\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ba66e9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:46:46.279779Z",
     "iopub.status.busy": "2022-06-19T05:46:46.279537Z",
     "iopub.status.idle": "2022-06-19T05:46:46.287133Z",
     "shell.execute_reply": "2022-06-19T05:46:46.286149Z"
    },
    "papermill": {
     "duration": 0.117973,
     "end_time": "2022-06-19T05:46:46.289434",
     "exception": false,
     "start_time": "2022-06-19T05:46:46.171461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerHead(nn.Module):\n",
    "    def __init__(self, in_features, max_length, num_layers=1, nhead=8, num_targets=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer=nn.TransformerEncoderLayer(d_model=in_features,\n",
    "                                                                                          nhead=nhead),\n",
    "                                                 num_layers=num_layers)\n",
    "        self.row_fc = nn.Linear(in_features, 1)\n",
    "        self.out_features = max_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.transformer(x)\n",
    "        out = self.row_fc(out).squeeze(-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54f4918c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:46:46.533187Z",
     "iopub.status.busy": "2022-06-19T05:46:46.532792Z",
     "iopub.status.idle": "2022-06-19T05:46:46.549120Z",
     "shell.execute_reply": "2022-06-19T05:46:46.548151Z"
    },
    "papermill": {
     "duration": 0.143963,
     "end_time": "2022-06-19T05:46:46.551281",
     "exception": false,
     "start_time": "2022-06-19T05:46:46.407318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel.from_config(self.config)\n",
    "        \n",
    "        self.feature_extractor = AutoModelForTokenClassification.from_pretrained(\"../input/deberta-v3-large/deberta-v3-large\")\n",
    "        in_features = self.feature_extractor.classifier.in_features\n",
    "        self.attention = TransformerHead(in_features=in_features, max_length=133, num_layers=1, nhead=8, num_targets=1)\n",
    "        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n",
    "        self.fc = nn.Linear(self.attention.out_features, self.cfg.target_size)\n",
    "        self._init_weights(self.fc)\n",
    "        self._init_weights(self.attention)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        # feature = torch.mean(last_hidden_states, 1)\n",
    "        feature = self.attention(last_hidden_states)\n",
    "        \n",
    "        return feature\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        #print(feature.shape)\n",
    "        output = self.fc(self.fc_dropout(feature))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eac4bcc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:46:46.767627Z",
     "iopub.status.busy": "2022-06-19T05:46:46.767367Z",
     "iopub.status.idle": "2022-06-19T05:46:46.774310Z",
     "shell.execute_reply": "2022-06-19T05:46:46.773353Z"
    },
    "papermill": {
     "duration": 0.117245,
     "end_time": "2022-06-19T05:46:46.776421",
     "exception": false,
     "start_time": "2022-06-19T05:46:46.659176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# inference\n",
    "# ====================================================\n",
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78d8459a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:46:46.994934Z",
     "iopub.status.busy": "2022-06-19T05:46:46.993471Z",
     "iopub.status.idle": "2022-06-19T05:51:14.614300Z",
     "shell.execute_reply": "2022-06-19T05:51:14.613118Z"
    },
    "papermill": {
     "duration": 267.82078,
     "end_time": "2022-06-19T05:51:14.705287",
     "exception": false,
     "start_time": "2022-06-19T05:46:46.884507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2ForTokenClassification: ['mask_predictions.classifer.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifer.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at ../input/deberta-v3-large/deberta-v3-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.22s/it]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2ForTokenClassification: ['mask_predictions.classifer.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifer.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at ../input/deberta-v3-large/deberta-v3-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.17it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2ForTokenClassification: ['mask_predictions.classifer.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifer.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at ../input/deberta-v3-large/deberta-v3-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.13it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2ForTokenClassification: ['mask_predictions.classifer.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifer.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at ../input/deberta-v3-large/deberta-v3-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TestDataset(CFG, test)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=CFG.batch_size,\n",
    "                         shuffle=False,\n",
    "                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "predictions = []\n",
    "for fold in CFG.trn_fold:\n",
    "    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n",
    "    state = torch.load(f\"../input/debertalargetransformerheadfold{fold}/uspppmmicrosoft-deberta-v3-large_fold{fold}_best.pth\",\n",
    "                       map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state['model'])\n",
    "    prediction = inference_fn(test_loader, model, device)\n",
    "    predictions.append(prediction)\n",
    "    del model, state, prediction; gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "pred1 = np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d2a14e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:51:14.956020Z",
     "iopub.status.busy": "2022-06-19T05:51:14.955715Z",
     "iopub.status.idle": "2022-06-19T05:51:14.961989Z",
     "shell.execute_reply": "2022-06-19T05:51:14.961007Z"
    },
    "papermill": {
     "duration": 0.134664,
     "end_time": "2022-06-19T05:51:14.964384",
     "exception": false,
     "start_time": "2022-06-19T05:51:14.829720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MMscaler = MinMaxScaler()\n",
    "pred1_mm = MMscaler.fit_transform(pred1.reshape(-1,1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "595ad521",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:51:15.222699Z",
     "iopub.status.busy": "2022-06-19T05:51:15.222426Z",
     "iopub.status.idle": "2022-06-19T05:51:15.244062Z",
     "shell.execute_reply": "2022-06-19T05:51:15.243048Z"
    },
    "papermill": {
     "duration": 0.157578,
     "end_time": "2022-06-19T05:51:15.246752",
     "exception": false,
     "start_time": "2022-06-19T05:51:15.089174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>0.498692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09e418c93a776564</td>\n",
       "      <td>0.722397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36baf228038e314b</td>\n",
       "      <td>0.449420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f37ead645e7f0c8</td>\n",
       "      <td>0.254730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71a5b6ad068d531f</td>\n",
       "      <td>0.002512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>474c874d0c07bd21</td>\n",
       "      <td>0.467280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>442c114ed5c4e3c9</td>\n",
       "      <td>0.459023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b8ae62ea5e1d8bdb</td>\n",
       "      <td>0.000562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>faaddaf8fcba8a3f</td>\n",
       "      <td>0.295239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ae0262c02566d2ce</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a8808e31641e856d</td>\n",
       "      <td>0.235569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16ae4b99d3601e60</td>\n",
       "      <td>0.279410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25c555ca3d5a2092</td>\n",
       "      <td>0.753422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5203a36c501f1b7c</td>\n",
       "      <td>0.944598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     score\n",
       "0   4112d61851461f60  0.498692\n",
       "1   09e418c93a776564  0.722397\n",
       "2   36baf228038e314b  0.449420\n",
       "3   1f37ead645e7f0c8  0.254730\n",
       "4   71a5b6ad068d531f  0.002512\n",
       "5   474c874d0c07bd21  0.467280\n",
       "6   442c114ed5c4e3c9  0.459023\n",
       "7   b8ae62ea5e1d8bdb  0.000562\n",
       "8   faaddaf8fcba8a3f  0.295239\n",
       "9   ae0262c02566d2ce  1.000000\n",
       "10  a8808e31641e856d  0.235569\n",
       "11  16ae4b99d3601e60  0.279410\n",
       "12  25c555ca3d5a2092  0.753422\n",
       "13  5203a36c501f1b7c  0.944598"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_1 = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'score': pred1_mm,\n",
    "})\n",
    "\n",
    "submission_1.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5801cfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:51:15.500226Z",
     "iopub.status.busy": "2022-06-19T05:51:15.499498Z",
     "iopub.status.idle": "2022-06-19T05:55:12.592190Z",
     "shell.execute_reply": "2022-06-19T05:55:12.590935Z"
    },
    "papermill": {
     "duration": 237.220666,
     "end_time": "2022-06-19T05:55:12.594381",
     "exception": false,
     "start_time": "2022-06-19T05:51:15.373715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2ForTokenClassification: ['mask_predictions.classifer.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifer.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at ../input/deberta-v3-large/deberta-v3-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.29it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2ForTokenClassification: ['mask_predictions.classifer.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifer.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at ../input/deberta-v3-large/deberta-v3-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.28it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2ForTokenClassification: ['mask_predictions.classifer.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifer.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at ../input/deberta-v3-large/deberta-v3-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.21it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2ForTokenClassification: ['mask_predictions.classifer.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifer.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at ../input/deberta-v3-large/deberta-v3-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TestDataset(CFG, test)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=CFG.batch_size,\n",
    "                         shuffle=False,\n",
    "                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "predictions = []\n",
    "for fold in CFG.trn_fold:\n",
    "    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n",
    "    state = torch.load(f\"../input/debertalargetransformerheadgkf{fold}/uspppmmicrosoft-deberta-v3-large_fold{fold}_best.pth\",\n",
    "                       map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state['model'])\n",
    "    prediction = inference_fn(test_loader, model, device)\n",
    "    predictions.append(prediction)\n",
    "    del model, state, prediction; gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "pred6 = np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9291684a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:55:12.863754Z",
     "iopub.status.busy": "2022-06-19T05:55:12.862868Z",
     "iopub.status.idle": "2022-06-19T05:55:12.870112Z",
     "shell.execute_reply": "2022-06-19T05:55:12.869162Z"
    },
    "papermill": {
     "duration": 0.144844,
     "end_time": "2022-06-19T05:55:12.872402",
     "exception": false,
     "start_time": "2022-06-19T05:55:12.727558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MMscaler = MinMaxScaler()\n",
    "pred6_mm = MMscaler.fit_transform(pred6.reshape(-1,1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e37a7344",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:55:13.138859Z",
     "iopub.status.busy": "2022-06-19T05:55:13.138580Z",
     "iopub.status.idle": "2022-06-19T05:55:13.157524Z",
     "shell.execute_reply": "2022-06-19T05:55:13.156585Z"
    },
    "papermill": {
     "duration": 0.154274,
     "end_time": "2022-06-19T05:55:13.159890",
     "exception": false,
     "start_time": "2022-06-19T05:55:13.005616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>0.511849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09e418c93a776564</td>\n",
       "      <td>0.734401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36baf228038e314b</td>\n",
       "      <td>0.479310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f37ead645e7f0c8</td>\n",
       "      <td>0.272812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71a5b6ad068d531f</td>\n",
       "      <td>0.002929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>474c874d0c07bd21</td>\n",
       "      <td>0.449986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>442c114ed5c4e3c9</td>\n",
       "      <td>0.442169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b8ae62ea5e1d8bdb</td>\n",
       "      <td>0.000617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>faaddaf8fcba8a3f</td>\n",
       "      <td>0.275198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ae0262c02566d2ce</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a8808e31641e856d</td>\n",
       "      <td>0.159763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16ae4b99d3601e60</td>\n",
       "      <td>0.286872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25c555ca3d5a2092</td>\n",
       "      <td>0.763868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5203a36c501f1b7c</td>\n",
       "      <td>0.848405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     score\n",
       "0   4112d61851461f60  0.511849\n",
       "1   09e418c93a776564  0.734401\n",
       "2   36baf228038e314b  0.479310\n",
       "3   1f37ead645e7f0c8  0.272812\n",
       "4   71a5b6ad068d531f  0.002929\n",
       "5   474c874d0c07bd21  0.449986\n",
       "6   442c114ed5c4e3c9  0.442169\n",
       "7   b8ae62ea5e1d8bdb  0.000617\n",
       "8   faaddaf8fcba8a3f  0.275198\n",
       "9   ae0262c02566d2ce  1.000000\n",
       "10  a8808e31641e856d  0.159763\n",
       "11  16ae4b99d3601e60  0.286872\n",
       "12  25c555ca3d5a2092  0.763868\n",
       "13  5203a36c501f1b7c  0.848405"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_6 = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'score': pred6_mm,\n",
    "})\n",
    "\n",
    "submission_6.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4184b3e",
   "metadata": {
    "papermill": {
     "duration": 0.154645,
     "end_time": "2022-06-19T05:55:13.449657",
     "exception": false,
     "start_time": "2022-06-19T05:55:13.295012",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.anferico/bert-for-patents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d47f136",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:55:13.991293Z",
     "iopub.status.busy": "2022-06-19T05:55:13.990965Z",
     "iopub.status.idle": "2022-06-19T05:55:14.000208Z",
     "shell.execute_reply": "2022-06-19T05:55:13.998983Z"
    },
    "papermill": {
     "duration": 0.246574,
     "end_time": "2022-06-19T05:55:14.002978",
     "exception": false,
     "start_time": "2022-06-19T05:55:13.756404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    fold_num = 5\n",
    "    input_path = '../input/us-patent-phrase-to-phrase-matching/'\n",
    "    model_path = '../input/patent-phrase-fold4/'\n",
    "    \n",
    "    learning_rate = 2e-5\n",
    "    weight_decay = 0.01\n",
    "    num_fold = 5\n",
    "    epochs = 5\n",
    "    batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44f08e1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:55:14.310613Z",
     "iopub.status.busy": "2022-06-19T05:55:14.310287Z",
     "iopub.status.idle": "2022-06-19T05:55:15.507779Z",
     "shell.execute_reply": "2022-06-19T05:55:15.506896Z"
    },
    "papermill": {
     "duration": 1.339382,
     "end_time": "2022-06-19T05:55:15.511237",
     "exception": false,
     "start_time": "2022-06-19T05:55:14.171855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(f\"{CFG.input_path}test.csv\")\n",
    "titles = pd.read_csv('../input/cpc-codes/titles.csv')\n",
    "test_df.reset_index(inplace=True)\n",
    "test_df = test_df.merge(titles, left_on='context', right_on='code')\n",
    "test_df.sort_values(by='index', inplace=True)\n",
    "test_df.reset_index(inplace=True)\n",
    "test_df.drop(columns='index', inplace=True)\n",
    "test_df['input'] = test_df['title']+' '+test_df['anchor']\n",
    "test_df = test_df.drop(columns=[\"context\", \"code\", \"class\", \"subclass\", \"group\", \"main_group\", \"anchor\", \"title\", \"section\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5daa022",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:55:15.957753Z",
     "iopub.status.busy": "2022-06-19T05:55:15.957394Z",
     "iopub.status.idle": "2022-06-19T05:55:16.040740Z",
     "shell.execute_reply": "2022-06-19T05:55:16.039546Z"
    },
    "papermill": {
     "duration": 0.311451,
     "end_time": "2022-06-19T05:55:16.044199",
     "exception": false,
     "start_time": "2022-06-19T05:55:15.732748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(f'{CFG.model_path}uspppm_SKF_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11578998",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:55:16.403953Z",
     "iopub.status.busy": "2022-06-19T05:55:16.403651Z",
     "iopub.status.idle": "2022-06-19T05:55:16.411538Z",
     "shell.execute_reply": "2022-06-19T05:55:16.410346Z"
    },
    "papermill": {
     "duration": 0.14885,
     "end_time": "2022-06-19T05:55:16.414442",
     "exception": false,
     "start_time": "2022-06-19T05:55:16.265592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InferDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.inputs = df['input'].values.astype(str)\n",
    "        self.targets = df['target'].values.astype(str)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = self.inputs[item]\n",
    "        targets = self.targets[item]\n",
    "        \n",
    "        return {\n",
    "        **tokenizer( inputs, targets )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb293f48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:55:16.690068Z",
     "iopub.status.busy": "2022-06-19T05:55:16.689756Z",
     "iopub.status.idle": "2022-06-19T05:56:28.398162Z",
     "shell.execute_reply": "2022-06-19T05:56:28.397023Z"
    },
    "papermill": {
     "duration": 71.847324,
     "end_time": "2022-06-19T05:56:28.400903",
     "exception": false,
     "start_time": "2022-06-19T05:55:16.553579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../input/patent-phrase-fold4/uspppm_SKF_1/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../input/patent-phrase-fold4/uspppm_SKF_1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 39859\n",
      "}\n",
      "\n",
      "loading weights file ../input/patent-phrase-fold4/uspppm_SKF_1/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ../input/patent-phrase-fold4/uspppm_SKF_1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../input/patent-phrase-fold4/uspppm_SKF_2/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../input/patent-phrase-fold4/uspppm_SKF_2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 39859\n",
      "}\n",
      "\n",
      "loading weights file ../input/patent-phrase-fold4/uspppm_SKF_2/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ../input/patent-phrase-fold4/uspppm_SKF_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../input/patent-phrase-fold4/uspppm_SKF_3/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../input/patent-phrase-fold4/uspppm_SKF_3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 39859\n",
      "}\n",
      "\n",
      "loading weights file ../input/patent-phrase-fold4/uspppm_SKF_3/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ../input/patent-phrase-fold4/uspppm_SKF_3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../input/patent-phrase-fold4/uspppm_SKF_4/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../input/patent-phrase-fold4/uspppm_SKF_4\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 39859\n",
      "}\n",
      "\n",
      "loading weights file ../input/patent-phrase-fold4/uspppm_SKF_4/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ../input/patent-phrase-fold4/uspppm_SKF_4.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for fold in range(CFG.num_fold):\n",
    "    te_dataset = InferDataset(test_df)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(f'{CFG.model_path}uspppm_SKF_{fold}', num_labels=1)\n",
    "    trainer = Trainer(\n",
    "            model,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "    outputs = trainer.predict(te_dataset)\n",
    "    prediction = outputs.predictions.reshape(-1)\n",
    "    predictions.append(prediction)\n",
    "    del model, prediction; gc.collect()\n",
    "    torch.cuda.empty_cache() \n",
    "pred2 = np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50a05421",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:56:28.716698Z",
     "iopub.status.busy": "2022-06-19T05:56:28.715708Z",
     "iopub.status.idle": "2022-06-19T05:56:28.722043Z",
     "shell.execute_reply": "2022-06-19T05:56:28.720821Z"
    },
    "papermill": {
     "duration": 0.166727,
     "end_time": "2022-06-19T05:56:28.724697",
     "exception": false,
     "start_time": "2022-06-19T05:56:28.557970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MMscaler = MinMaxScaler()\n",
    "pred2_mm = MMscaler.fit_transform(pred2.reshape(-1,1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d367542",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:56:29.039900Z",
     "iopub.status.busy": "2022-06-19T05:56:29.039642Z",
     "iopub.status.idle": "2022-06-19T05:56:29.053788Z",
     "shell.execute_reply": "2022-06-19T05:56:29.052773Z"
    },
    "papermill": {
     "duration": 0.175087,
     "end_time": "2022-06-19T05:56:29.055927",
     "exception": false,
     "start_time": "2022-06-19T05:56:28.880840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>0.458402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09e418c93a776564</td>\n",
       "      <td>0.605043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36baf228038e314b</td>\n",
       "      <td>0.436031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f37ead645e7f0c8</td>\n",
       "      <td>0.238926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71a5b6ad068d531f</td>\n",
       "      <td>0.000457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>474c874d0c07bd21</td>\n",
       "      <td>0.476554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>442c114ed5c4e3c9</td>\n",
       "      <td>0.381174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b8ae62ea5e1d8bdb</td>\n",
       "      <td>0.002258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>faaddaf8fcba8a3f</td>\n",
       "      <td>0.251393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ae0262c02566d2ce</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a8808e31641e856d</td>\n",
       "      <td>0.232104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16ae4b99d3601e60</td>\n",
       "      <td>0.244893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25c555ca3d5a2092</td>\n",
       "      <td>0.772619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5203a36c501f1b7c</td>\n",
       "      <td>0.832154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     score\n",
       "0   4112d61851461f60  0.458402\n",
       "1   09e418c93a776564  0.605043\n",
       "2   36baf228038e314b  0.436031\n",
       "3   1f37ead645e7f0c8  0.238926\n",
       "4   71a5b6ad068d531f  0.000457\n",
       "5   474c874d0c07bd21  0.476554\n",
       "6   442c114ed5c4e3c9  0.381174\n",
       "7   b8ae62ea5e1d8bdb  0.002258\n",
       "8   faaddaf8fcba8a3f  0.251393\n",
       "9   ae0262c02566d2ce  1.000000\n",
       "10  a8808e31641e856d  0.232104\n",
       "11  16ae4b99d3601e60  0.244893\n",
       "12  25c555ca3d5a2092  0.772619\n",
       "13  5203a36c501f1b7c  0.832154"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_2 = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'score': pred2_mm,\n",
    "})\n",
    "\n",
    "submission_2.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32117d93",
   "metadata": {
    "papermill": {
     "duration": 0.15543,
     "end_time": "2022-06-19T05:56:29.368707",
     "exception": false,
     "start_time": "2022-06-19T05:56:29.213277",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.anferico/bert-for-patents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce05e747",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:56:29.685983Z",
     "iopub.status.busy": "2022-06-19T05:56:29.685113Z",
     "iopub.status.idle": "2022-06-19T05:56:29.689829Z",
     "shell.execute_reply": "2022-06-19T05:56:29.688860Z"
    },
    "papermill": {
     "duration": 0.162624,
     "end_time": "2022-06-19T05:56:29.692230",
     "exception": false,
     "start_time": "2022-06-19T05:56:29.529606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../input/tez-lib/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbc4c17d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:56:30.003711Z",
     "iopub.status.busy": "2022-06-19T05:56:30.002776Z",
     "iopub.status.idle": "2022-06-19T05:56:30.124759Z",
     "shell.execute_reply": "2022-06-19T05:56:30.123729Z"
    },
    "papermill": {
     "duration": 0.281501,
     "end_time": "2022-06-19T05:56:30.127508",
     "exception": false,
     "start_time": "2022-06-19T05:56:29.846007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from scipy import stats\n",
    "from tez import Tez, TezConfig\n",
    "from tez.callbacks import EarlyStopping\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d6ab68d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:56:30.477720Z",
     "iopub.status.busy": "2022-06-19T05:56:30.476489Z",
     "iopub.status.idle": "2022-06-19T05:56:30.484776Z",
     "shell.execute_reply": "2022-06-19T05:56:30.483535Z"
    },
    "papermill": {
     "duration": 0.207633,
     "end_time": "2022-06-19T05:56:30.491254",
     "exception": false,
     "start_time": "2022-06-19T05:56:30.283621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    model = \"../input/anferico-bert-for-patents/\"\n",
    "    max_len = 32\n",
    "    accumulation_steps = 1\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad44fd00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:56:31.072082Z",
     "iopub.status.busy": "2022-06-19T05:56:31.071727Z",
     "iopub.status.idle": "2022-06-19T05:56:31.091060Z",
     "shell.execute_reply": "2022-06-19T05:56:31.089880Z"
    },
    "papermill": {
     "duration": 0.297862,
     "end_time": "2022-06-19T05:56:31.093919",
     "exception": false,
     "start_time": "2022-06-19T05:56:30.796057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PhraseDataset:\n",
    "    def __init__(self, anchor, target, context, tokenizer, max_len):\n",
    "        self.anchor = anchor\n",
    "        self.target = target\n",
    "        self.context = context\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anchor)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        anchor = self.anchor[item]\n",
    "        context = self.context[item]\n",
    "        target = self.target[item]\n",
    "\n",
    "        encoded_text = self.tokenizer.encode_plus(\n",
    "            context + \" \" + anchor,\n",
    "            target,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "        )\n",
    "        input_ids = encoded_text[\"input_ids\"]\n",
    "        attention_mask = encoded_text[\"attention_mask\"]\n",
    "        token_type_ids = encoded_text[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37c936d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:56:31.433827Z",
     "iopub.status.busy": "2022-06-19T05:56:31.433563Z",
     "iopub.status.idle": "2022-06-19T05:56:31.442460Z",
     "shell.execute_reply": "2022-06-19T05:56:31.441492Z"
    },
    "papermill": {
     "duration": 0.183438,
     "end_time": "2022-06-19T05:56:31.444670",
     "exception": false,
     "start_time": "2022-06-19T05:56:31.261232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PhraseModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.update(\n",
    "            {\n",
    "                \"output_hidden_states\": True,\n",
    "                \"add_pooling_layer\": True,\n",
    "                \"num_labels\": 1,\n",
    "            }\n",
    "        )\n",
    "        self.transformer = AutoModel.from_pretrained(model_name, config=config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.output = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        transformer_out = self.transformer(ids, mask, token_type_ids)\n",
    "        output = transformer_out.pooler_output\n",
    "        output = self.dropout(output)\n",
    "        output = self.output(output)\n",
    "        return output, 0, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d3569b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:56:31.762480Z",
     "iopub.status.busy": "2022-06-19T05:56:31.762111Z",
     "iopub.status.idle": "2022-06-19T05:56:31.898384Z",
     "shell.execute_reply": "2022-06-19T05:56:31.896761Z"
    },
    "papermill": {
     "duration": 0.298788,
     "end_time": "2022-06-19T05:56:31.900750",
     "exception": false,
     "start_time": "2022-06-19T05:56:31.601962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file ../input/anferico-bert-for-patents/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../input/anferico-bert-for-patents/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 39859\n",
      "}\n",
      "\n",
      "Didn't find file ../input/anferico-bert-for-patents/tokenizer.json. We won't load it.\n",
      "Didn't find file ../input/anferico-bert-for-patents/added_tokens.json. We won't load it.\n",
      "Didn't find file ../input/anferico-bert-for-patents/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ../input/anferico-bert-for-patents/tokenizer_config.json. We won't load it.\n",
      "loading file ../input/anferico-bert-for-patents/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file ../input/anferico-bert-for-patents/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../input/anferico-bert-for-patents/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 39859\n",
      "}\n",
      "\n",
      "loading configuration file ../input/anferico-bert-for-patents/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../input/anferico-bert-for-patents/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 39859\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/test.csv\")\n",
    "\n",
    "context_mapping = {\n",
    "    \"A\": \"Human Necessities\",\n",
    "    \"B\": \"Operations and Transport\",\n",
    "    \"C\": \"Chemistry and Metallurgy\",\n",
    "    \"D\": \"Textiles\",\n",
    "    \"E\": \"Fixed Constructions\",\n",
    "    \"F\": \"Mechanical Engineering\",\n",
    "    \"G\": \"Physics\",\n",
    "    \"H\": \"Electricity\",\n",
    "    \"Y\": \"Emerging Cross-Sectional Technologies\",\n",
    "}\n",
    "\n",
    "df.context = df.context.apply(lambda x: context_mapping[x[0]])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model)\n",
    "test_dataset = PhraseDataset(\n",
    "    anchor=df.anchor.values,\n",
    "    target=df.target.values,\n",
    "    context=df.context.values,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=args.max_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3f61bde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:56:32.230574Z",
     "iopub.status.busy": "2022-06-19T05:56:32.230292Z",
     "iopub.status.idle": "2022-06-19T05:58:48.698843Z",
     "shell.execute_reply": "2022-06-19T05:58:48.697712Z"
    },
    "papermill": {
     "duration": 136.634555,
     "end_time": "2022-06-19T05:58:48.701639",
     "exception": false,
     "start_time": "2022-06-19T05:56:32.067084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../input/anferico-bert-for-patents/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../input/anferico-bert-for-patents/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 39859\n",
      "}\n",
      "\n",
      "loading weights file ../input/anferico-bert-for-patents/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ../input/anferico-bert-for-patents/ were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at ../input/anferico-bert-for-patents/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file ../input/anferico-bert-for-patents/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../input/anferico-bert-for-patents/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 39859\n",
      "}\n",
      "\n",
      "loading weights file ../input/anferico-bert-for-patents/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ../input/anferico-bert-for-patents/ were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at ../input/anferico-bert-for-patents/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file ../input/anferico-bert-for-patents/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../input/anferico-bert-for-patents/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 39859\n",
      "}\n",
      "\n",
      "loading weights file ../input/anferico-bert-for-patents/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ../input/anferico-bert-for-patents/ were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at ../input/anferico-bert-for-patents/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file ../input/anferico-bert-for-patents/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../input/anferico-bert-for-patents/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 39859\n",
      "}\n",
      "\n",
      "loading weights file ../input/anferico-bert-for-patents/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ../input/anferico-bert-for-patents/ were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at ../input/anferico-bert-for-patents/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file ../input/anferico-bert-for-patents/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../input/anferico-bert-for-patents/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 39859\n",
      "}\n",
      "\n",
      "loading weights file ../input/anferico-bert-for-patents/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ../input/anferico-bert-for-patents/ were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at ../input/anferico-bert-for-patents/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for model_index in range(5):\n",
    "    model = PhraseModel(model_name=args.model)\n",
    "    model = Tez(model)\n",
    "    model_path = f\"../input/usppmtez5folds0526/model_f{model_index}.bin\"\n",
    "    config = TezConfig(\n",
    "        test_batch_size=64,\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "    model.load(model_path, weights_only=True, config=config)\n",
    "\n",
    "    preds_iter = model.predict(test_dataset)\n",
    "    final_preds = []\n",
    "    for preds in preds_iter:\n",
    "#        preds[preds < 0] = 0\n",
    "#        preds[preds > 1] = 1\n",
    "        final_preds.extend(preds.ravel().tolist())\n",
    "    predictions.append(np.array(final_preds))\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "pred3 = np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d3a5ecb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:58:49.048953Z",
     "iopub.status.busy": "2022-06-19T05:58:49.048676Z",
     "iopub.status.idle": "2022-06-19T05:58:49.054254Z",
     "shell.execute_reply": "2022-06-19T05:58:49.053298Z"
    },
    "papermill": {
     "duration": 0.183071,
     "end_time": "2022-06-19T05:58:49.056443",
     "exception": false,
     "start_time": "2022-06-19T05:58:48.873372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MMscaler = MinMaxScaler()\n",
    "pred3_mm = MMscaler.fit_transform(pred3.reshape(-1,1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "701cc539",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:58:49.406086Z",
     "iopub.status.busy": "2022-06-19T05:58:49.405085Z",
     "iopub.status.idle": "2022-06-19T05:58:49.418652Z",
     "shell.execute_reply": "2022-06-19T05:58:49.417650Z"
    },
    "papermill": {
     "duration": 0.189579,
     "end_time": "2022-06-19T05:58:49.421610",
     "exception": false,
     "start_time": "2022-06-19T05:58:49.232031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>0.475254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09e418c93a776564</td>\n",
       "      <td>0.608357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36baf228038e314b</td>\n",
       "      <td>0.398566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f37ead645e7f0c8</td>\n",
       "      <td>0.235248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71a5b6ad068d531f</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>474c874d0c07bd21</td>\n",
       "      <td>0.461583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>442c114ed5c4e3c9</td>\n",
       "      <td>0.334973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b8ae62ea5e1d8bdb</td>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>faaddaf8fcba8a3f</td>\n",
       "      <td>0.214788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ae0262c02566d2ce</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a8808e31641e856d</td>\n",
       "      <td>0.256813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16ae4b99d3601e60</td>\n",
       "      <td>0.237898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25c555ca3d5a2092</td>\n",
       "      <td>0.729493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5203a36c501f1b7c</td>\n",
       "      <td>0.773707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     score\n",
       "0   4112d61851461f60  0.475254\n",
       "1   09e418c93a776564  0.608357\n",
       "2   36baf228038e314b  0.398566\n",
       "3   1f37ead645e7f0c8  0.235248\n",
       "4   71a5b6ad068d531f  0.000000\n",
       "5   474c874d0c07bd21  0.461583\n",
       "6   442c114ed5c4e3c9  0.334973\n",
       "7   b8ae62ea5e1d8bdb  0.000103\n",
       "8   faaddaf8fcba8a3f  0.214788\n",
       "9   ae0262c02566d2ce  1.000000\n",
       "10  a8808e31641e856d  0.256813\n",
       "11  16ae4b99d3601e60  0.237898\n",
       "12  25c555ca3d5a2092  0.729493\n",
       "13  5203a36c501f1b7c  0.773707"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_3 = pd.DataFrame({\n",
    "    'id': df['id'],\n",
    "    'score': pred3_mm,\n",
    "})\n",
    "\n",
    "submission_3.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab578484",
   "metadata": {
    "papermill": {
     "duration": 0.174182,
     "end_time": "2022-06-19T05:58:49.769864",
     "exception": false,
     "start_time": "2022-06-19T05:58:49.595682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4.Roberta-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "319703e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:58:50.117779Z",
     "iopub.status.busy": "2022-06-19T05:58:50.116941Z",
     "iopub.status.idle": "2022-06-19T05:58:50.124921Z",
     "shell.execute_reply": "2022-06-19T05:58:50.123868Z"
    },
    "papermill": {
     "duration": 0.184715,
     "end_time": "2022-06-19T05:58:50.127074",
     "exception": false,
     "start_time": "2022-06-19T05:58:49.942359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(seed=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "424fa830",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:58:50.472762Z",
     "iopub.status.busy": "2022-06-19T05:58:50.472479Z",
     "iopub.status.idle": "2022-06-19T05:58:50.755455Z",
     "shell.execute_reply": "2022-06-19T05:58:50.754054Z"
    },
    "papermill": {
     "duration": 0.460324,
     "end_time": "2022-06-19T05:58:50.757645",
     "exception": false,
     "start_time": "2022-06-19T05:58:50.297321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file ../input/robertalarge/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../input/robertalarge\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Didn't find file ../input/robertalarge/tokenizer.json. We won't load it.\n",
      "Didn't find file ../input/robertalarge/added_tokens.json. We won't load it.\n",
      "Didn't find file ../input/robertalarge/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ../input/robertalarge/tokenizer_config.json. We won't load it.\n",
      "loading file ../input/robertalarge/vocab.json\n",
      "loading file ../input/robertalarge/merges.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file ../input/robertalarge/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../input/robertalarge\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file ../input/robertalarge/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../input/robertalarge\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass(frozen=True)\n",
    "class CFG:\n",
    "    num_workers: Optional[int] = 4\n",
    "    config_path: Optional[str] = '../input/robertalarge'\n",
    "    model_path: Optional[str] = '../input/220606-uspppm-roberta7'\n",
    "    model_name: Optional[str] = 'roberta-large'\n",
    "    batch_size: Optional[int] = 32\n",
    "    max_len: Optional[int] = 128\n",
    "    seed: Optional[int] = 2019\n",
    "    num_targets: Optional[int] = 1\n",
    "    n_folds: Optional[int] = 4\n",
    "    tokenizer = AutoTokenizer.from_pretrained('../input/robertalarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f07d668",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:58:51.122043Z",
     "iopub.status.busy": "2022-06-19T05:58:51.121284Z",
     "iopub.status.idle": "2022-06-19T05:58:51.130040Z",
     "shell.execute_reply": "2022-06-19T05:58:51.129142Z"
    },
    "papermill": {
     "duration": 0.192652,
     "end_time": "2022-06-19T05:58:51.132467",
     "exception": false,
     "start_time": "2022-06-19T05:58:50.939815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH = '../input/us-patent-phrase-to-phrase-matching'\n",
    "test = pd.read_csv(os.path.join(PATH, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9120b1ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:58:51.541290Z",
     "iopub.status.busy": "2022-06-19T05:58:51.540938Z",
     "iopub.status.idle": "2022-06-19T05:58:51.549435Z",
     "shell.execute_reply": "2022-06-19T05:58:51.548534Z"
    },
    "papermill": {
     "duration": 0.239014,
     "end_time": "2022-06-19T05:58:51.551720",
     "exception": false,
     "start_time": "2022-06-19T05:58:51.312706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "context_mapping = {\n",
    "        \"A\": \"Human Necessities\",\n",
    "        \"B\": \"Operations and Transport\",\n",
    "        \"C\": \"Chemistry and Metallurgy\",\n",
    "        \"D\": \"Textiles\",\n",
    "        \"E\": \"Fixed Constructions\",\n",
    "        \"F\": \"Mechanical Engineering\",\n",
    "        \"G\": \"Physics\",\n",
    "        \"H\": \"Electricity\",\n",
    "        \"Y\": \"Emerging Cross-Sectional Technologies\",\n",
    "}\n",
    "    \n",
    "test.context = test.context.apply(lambda x: context_mapping[x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d35e291e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:58:51.908049Z",
     "iopub.status.busy": "2022-06-19T05:58:51.907731Z",
     "iopub.status.idle": "2022-06-19T05:58:51.917345Z",
     "shell.execute_reply": "2022-06-19T05:58:51.916118Z"
    },
    "papermill": {
     "duration": 0.190727,
     "end_time": "2022-06-19T05:58:51.920486",
     "exception": false,
     "start_time": "2022-06-19T05:58:51.729759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PhraseDataset:\n",
    "    def __init__(self, anchor, target, context, tokenizer, max_len):\n",
    "        self.anchor = anchor\n",
    "        self.target = target\n",
    "        self.context = context\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anchor)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        anchor = self.anchor[item]\n",
    "        context = self.context[item]\n",
    "        target = self.target[item]\n",
    "\n",
    "        encoded_text = CFG.tokenizer.encode_plus(\n",
    "            context + \" \" + anchor,\n",
    "            target,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "        )\n",
    "        input_ids = encoded_text[\"input_ids\"]\n",
    "        attention_mask = encoded_text[\"attention_mask\"]\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2a8a85e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:58:52.279501Z",
     "iopub.status.busy": "2022-06-19T05:58:52.279066Z",
     "iopub.status.idle": "2022-06-19T05:58:52.287661Z",
     "shell.execute_reply": "2022-06-19T05:58:52.286682Z"
    },
    "papermill": {
     "duration": 0.189354,
     "end_time": "2022-06-19T05:58:52.289760",
     "exception": false,
     "start_time": "2022-06-19T05:58:52.100406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference_fn(model, test_loader):  \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for data in tk0:\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(ids, mask)\n",
    "        predictions.append(output.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    return np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a02f34e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:58:52.648899Z",
     "iopub.status.busy": "2022-06-19T05:58:52.648620Z",
     "iopub.status.idle": "2022-06-19T05:58:52.660806Z",
     "shell.execute_reply": "2022-06-19T05:58:52.659661Z"
    },
    "papermill": {
     "duration": 0.19426,
     "end_time": "2022-06-19T05:58:52.663670",
     "exception": false,
     "start_time": "2022-06-19T05:58:52.469410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatentModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PatentModel, self).__init__()\n",
    "        hidden_dropout_prob: float = 0.1\n",
    "        layer_norm_eps: float = 1e-7\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.config_path)\n",
    "\n",
    "        config.update(\n",
    "            {\n",
    "                \"output_hidden_states\": True,\n",
    "                \"hidden_dropout_prob\": hidden_dropout_prob,\n",
    "                \"layer_norm_eps\": layer_norm_eps,\n",
    "                \"add_pooling_layer\": False,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(CFG.config_path, config=config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        self.output = nn.Linear(config.hidden_size, CFG.num_targets)\n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        transformer_out = self.transformer(input_ids=ids, attention_mask=mask)\n",
    "        last_hidden_states = transformer_out[0]\n",
    "        last_hidden_states = self.dropout(torch.mean(last_hidden_states, 1))\n",
    "        logits1 = self.output(self.dropout1(last_hidden_states))\n",
    "        logits2 = self.output(self.dropout2(last_hidden_states))\n",
    "        logits3 = self.output(self.dropout3(last_hidden_states))\n",
    "        logits4 = self.output(self.dropout4(last_hidden_states))\n",
    "        logits5 = self.output(self.dropout5(last_hidden_states))\n",
    "        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c75abc17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:58:53.023764Z",
     "iopub.status.busy": "2022-06-19T05:58:53.023485Z",
     "iopub.status.idle": "2022-06-19T05:58:53.032765Z",
     "shell.execute_reply": "2022-06-19T05:58:53.031784Z"
    },
    "papermill": {
     "duration": 0.193948,
     "end_time": "2022-06-19T05:58:53.035059",
     "exception": false,
     "start_time": "2022-06-19T05:58:52.841111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_fold(test, fold, seed=42):    \n",
    "    \n",
    "    seed_everything(seed)\n",
    "    \n",
    "    test_dataset = PhraseDataset(\n",
    "        test.anchor.values,\n",
    "        test.target.values,\n",
    "        test.context.values,\n",
    "        CFG.tokenizer, \n",
    "        CFG.max_len\n",
    "    ) \n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, \n",
    "                              batch_size=CFG.batch_size * 2, \n",
    "                              shuffle=False, \n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    model = PatentModel()\n",
    "    \n",
    "    model.load_state_dict(\n",
    "        torch.load(f'{CFG.model_path}/{CFG.model_name.replace(\"-\",\"_\")}_patent_model_{fold}.pth',\n",
    "        map_location=torch.device('cuda')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    preds = inference_fn(model, test_loader)\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7893369",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:58:53.401775Z",
     "iopub.status.busy": "2022-06-19T05:58:53.400831Z",
     "iopub.status.idle": "2022-06-19T05:58:53.416247Z",
     "shell.execute_reply": "2022-06-19T05:58:53.415243Z"
    },
    "papermill": {
     "duration": 0.203174,
     "end_time": "2022-06-19T05:58:53.419328",
     "exception": false,
     "start_time": "2022-06-19T05:58:53.216154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference_model(test, seed):\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for f in range(CFG.n_folds):    \n",
    "        preds = run_fold(test, f, seed) \n",
    "        predictions.append(preds)\n",
    "        \n",
    "    test_preds = np.column_stack(predictions)\n",
    "        \n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9e0d357",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T05:58:54.084677Z",
     "iopub.status.busy": "2022-06-19T05:58:54.084356Z",
     "iopub.status.idle": "2022-06-19T06:00:50.829110Z",
     "shell.execute_reply": "2022-06-19T06:00:50.827801Z"
    },
    "papermill": {
     "duration": 117.055323,
     "end_time": "2022-06-19T06:00:50.831646",
     "exception": false,
     "start_time": "2022-06-19T05:58:53.776323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../input/robertalarge/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../input/robertalarge\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ../input/robertalarge/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ../input/robertalarge were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of RobertaModel were initialized from the model checkpoint at ../input/robertalarge.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "loading configuration file ../input/robertalarge/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../input/robertalarge\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ../input/robertalarge/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ../input/robertalarge were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of RobertaModel were initialized from the model checkpoint at ../input/robertalarge.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
      "loading configuration file ../input/robertalarge/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../input/robertalarge\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ../input/robertalarge/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ../input/robertalarge were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of RobertaModel were initialized from the model checkpoint at ../input/robertalarge.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "loading configuration file ../input/robertalarge/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../input/robertalarge\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ../input/robertalarge/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ../input/robertalarge were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of RobertaModel were initialized from the model checkpoint at ../input/robertalarge.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    pred4 =  np.mean(inference_model(test, CFG.seed),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c357200",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:00:51.229895Z",
     "iopub.status.busy": "2022-06-19T06:00:51.229549Z",
     "iopub.status.idle": "2022-06-19T06:00:51.235855Z",
     "shell.execute_reply": "2022-06-19T06:00:51.234757Z"
    },
    "papermill": {
     "duration": 0.208984,
     "end_time": "2022-06-19T06:00:51.238273",
     "exception": false,
     "start_time": "2022-06-19T06:00:51.029289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MMscaler = MinMaxScaler()\n",
    "pred4_mm = MMscaler.fit_transform(pred4.reshape(-1,1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "51063ddc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:00:51.674999Z",
     "iopub.status.busy": "2022-06-19T06:00:51.674629Z",
     "iopub.status.idle": "2022-06-19T06:00:51.689522Z",
     "shell.execute_reply": "2022-06-19T06:00:51.688323Z"
    },
    "papermill": {
     "duration": 0.212828,
     "end_time": "2022-06-19T06:00:51.691749",
     "exception": false,
     "start_time": "2022-06-19T06:00:51.478921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>0.560307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09e418c93a776564</td>\n",
       "      <td>0.757374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36baf228038e314b</td>\n",
       "      <td>0.445688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f37ead645e7f0c8</td>\n",
       "      <td>0.309286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71a5b6ad068d531f</td>\n",
       "      <td>0.064136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>474c874d0c07bd21</td>\n",
       "      <td>0.481080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>442c114ed5c4e3c9</td>\n",
       "      <td>0.446558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b8ae62ea5e1d8bdb</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>faaddaf8fcba8a3f</td>\n",
       "      <td>0.361119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ae0262c02566d2ce</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a8808e31641e856d</td>\n",
       "      <td>0.299794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16ae4b99d3601e60</td>\n",
       "      <td>0.277967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25c555ca3d5a2092</td>\n",
       "      <td>0.698386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5203a36c501f1b7c</td>\n",
       "      <td>0.738734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     score\n",
       "0   4112d61851461f60  0.560307\n",
       "1   09e418c93a776564  0.757374\n",
       "2   36baf228038e314b  0.445688\n",
       "3   1f37ead645e7f0c8  0.309286\n",
       "4   71a5b6ad068d531f  0.064136\n",
       "5   474c874d0c07bd21  0.481080\n",
       "6   442c114ed5c4e3c9  0.446558\n",
       "7   b8ae62ea5e1d8bdb  0.000065\n",
       "8   faaddaf8fcba8a3f  0.361119\n",
       "9   ae0262c02566d2ce  1.000000\n",
       "10  a8808e31641e856d  0.299794\n",
       "11  16ae4b99d3601e60  0.277967\n",
       "12  25c555ca3d5a2092  0.698386\n",
       "13  5203a36c501f1b7c  0.738734"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_4 = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'score': pred4_mm,\n",
    "})\n",
    "\n",
    "submission_4.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b4a675",
   "metadata": {
    "papermill": {
     "duration": 0.193217,
     "end_time": "2022-06-19T06:00:52.079451",
     "exception": false,
     "start_time": "2022-06-19T06:00:51.886234",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5.deberta_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "755c9f77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:00:52.474969Z",
     "iopub.status.busy": "2022-06-19T06:00:52.474199Z",
     "iopub.status.idle": "2022-06-19T06:00:52.481072Z",
     "shell.execute_reply": "2022-06-19T06:00:52.480118Z"
    },
    "papermill": {
     "duration": 0.208127,
     "end_time": "2022-06-19T06:00:52.483357",
     "exception": false,
     "start_time": "2022-06-19T06:00:52.275230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG_DEB_SIMPLE:\n",
    "    input_path = '../input/us-patent-phrase-to-phrase-matching/'\n",
    "    model_path = '../input/deberta-v3-large/deberta-v3-large'\n",
    "    batch_size = 24\n",
    "    num_workers = 2\n",
    "    num_fold = 4\n",
    "    max_input_length = 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fcda440b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:00:52.871683Z",
     "iopub.status.busy": "2022-06-19T06:00:52.871382Z",
     "iopub.status.idle": "2022-06-19T06:00:52.883753Z",
     "shell.execute_reply": "2022-06-19T06:00:52.882748Z"
    },
    "papermill": {
     "duration": 0.21044,
     "end_time": "2022-06-19T06:00:52.885907",
     "exception": false,
     "start_time": "2022-06-19T06:00:52.675467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_input_length):\n",
    "        self.text = df['text'].values.astype(str)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = self.text[item]\n",
    "        \n",
    "        inputs = self.tokenizer(inputs,\n",
    "                    max_length=self.max_input_length,\n",
    "                    padding='max_length',\n",
    "                    truncation=True)\n",
    "        \n",
    "        return torch.as_tensor(inputs['input_ids'], dtype=torch.long), \\\n",
    "               torch.as_tensor(inputs['token_type_ids'], dtype=torch.long), \\\n",
    "               torch.as_tensor(inputs['attention_mask'], dtype=torch.long)\n",
    "    \n",
    "    \n",
    "class Custom_Bert_Simple(nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super().__init__()\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(model_path)\n",
    "        config.num_labels = 1\n",
    "        self.base = AutoModelForSequenceClassification.from_config(config=config)\n",
    "        dim = config.hidden_size\n",
    "        self.dropout = nn.Dropout(p=0)\n",
    "        self.cls = nn.Linear(dim,1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        return base_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8b512c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:00:53.286260Z",
     "iopub.status.busy": "2022-06-19T06:00:53.285417Z",
     "iopub.status.idle": "2022-06-19T06:00:53.304871Z",
     "shell.execute_reply": "2022-06-19T06:00:53.303910Z"
    },
    "papermill": {
     "duration": 0.222835,
     "end_time": "2022-06-19T06:00:53.307084",
     "exception": false,
     "start_time": "2022-06-19T06:00:53.084249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    \n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        input_ids, token_type_ids, attention_mask = [i.to(device) for i in batch]\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            y_preds = model(input_ids, attention_mask, token_type_ids)\n",
    "        \n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "    \n",
    "    predictions = np.concatenate(preds)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "def upd_outputs(data, is_trim=True, is_minmax=True, is_reshape=True):\n",
    "    \"\"\"\\o/\"\"\"\n",
    "    if is_trim == True:\n",
    "        data = np.where(data <=0, 0, data)\n",
    "        data = np.where(data >=1, 1, data)\n",
    "\n",
    "    if is_minmax ==True:\n",
    "        data = min_max_scaler.fit_transform(data)\n",
    "    \n",
    "    if is_reshape == True:\n",
    "        data = data.reshape(-1)\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def _upd_score_between(data, thresholds, value):\n",
    "    \"\"\"\\o/\"\"\"\n",
    "    mask_th = data.between(*thresholds, inclusive='both')\n",
    "    data[mask_th] = value\n",
    "\n",
    "\n",
    "def upd_score(data, th_dict=None):\n",
    "    \"\"\"\\o/\"\"\"\n",
    "    if isinstance(data, pd.Series):\n",
    "        result = data.copy()\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "    if not th_dict:        \n",
    "        th_dict = {\n",
    "            '0': 0.02,\n",
    "            '.25': (0.24, 0.26),\n",
    "            '.50': (0.49, 0.51),\n",
    "            '.75': (0.74, 0.76),\n",
    "            '1': 0.98\n",
    "        }\n",
    "\n",
    "    if isinstance(th_dict, dict):    \n",
    "        th0 = th_dict.get('0')\n",
    "        th25 = th_dict.get('.25')\n",
    "        th50 = th_dict.get('.50')\n",
    "        th75 = th_dict.get('.75')\n",
    "        th100 = th_dict.get('1')\n",
    "    else:\n",
    "        return data\n",
    "    \n",
    "    if th0:\n",
    "        if isinstance(th0, float):\n",
    "            th0 = (result.min(), th0)\n",
    "        \n",
    "        if isinstance(th0, tuple):\n",
    "            _upd_score_between(result, th0, 0)\n",
    "    \n",
    "    if th25 and isinstance(th25, tuple):\n",
    "        _upd_score_between(result, th25, 0.25)\n",
    "\n",
    "    if th50 and isinstance(th50, tuple):\n",
    "        _upd_score_between(result, th50, 0.50)\n",
    "            \n",
    "    if th75 and isinstance(th75, tuple):\n",
    "        _upd_score_between(result, th75, 0.75)\n",
    "            \n",
    "    if th100:\n",
    "        if isinstance(th100, float):\n",
    "            th100 = (th100, result.max())\n",
    "        \n",
    "        if isinstance(th100, tuple):\n",
    "            _upd_score_between(result, th100, 1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2e986ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:00:53.702258Z",
     "iopub.status.busy": "2022-06-19T06:00:53.701423Z",
     "iopub.status.idle": "2022-06-19T06:00:53.735917Z",
     "shell.execute_reply": "2022-06-19T06:00:53.734974Z"
    },
    "papermill": {
     "duration": 0.231486,
     "end_time": "2022-06-19T06:00:53.738356",
     "exception": false,
     "start_time": "2022-06-19T06:00:53.506870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>context_text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>opc drum</td>\n",
       "      <td>inorganic photoconductor drum</td>\n",
       "      <td>G02</td>\n",
       "      <td>PHYSICS. OPTICS</td>\n",
       "      <td>opc drum[sep]inorganic photoconductor drum[sep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09e418c93a776564</td>\n",
       "      <td>adjust gas flow</td>\n",
       "      <td>altering gas flow</td>\n",
       "      <td>F23</td>\n",
       "      <td>MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...</td>\n",
       "      <td>adjust gas flow[sep]altering gas flow[sep]mech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36baf228038e314b</td>\n",
       "      <td>lower trunnion</td>\n",
       "      <td>lower locating</td>\n",
       "      <td>B60</td>\n",
       "      <td>PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...</td>\n",
       "      <td>lower trunnion[sep]lower locating[sep]performi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f37ead645e7f0c8</td>\n",
       "      <td>cap component</td>\n",
       "      <td>upper portion</td>\n",
       "      <td>D06</td>\n",
       "      <td>TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...</td>\n",
       "      <td>cap component[sep]upper portion[sep]textiles; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71a5b6ad068d531f</td>\n",
       "      <td>neural stimulation</td>\n",
       "      <td>artificial neural network</td>\n",
       "      <td>H04</td>\n",
       "      <td>ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE</td>\n",
       "      <td>neural stimulation[sep]artificial neural netwo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id              anchor                         target  \\\n",
       "0  4112d61851461f60            opc drum  inorganic photoconductor drum   \n",
       "1  09e418c93a776564     adjust gas flow              altering gas flow   \n",
       "2  36baf228038e314b      lower trunnion                 lower locating   \n",
       "3  1f37ead645e7f0c8       cap component                  upper portion   \n",
       "4  71a5b6ad068d531f  neural stimulation      artificial neural network   \n",
       "\n",
       "  context                                       context_text  \\\n",
       "0     G02                                    PHYSICS. OPTICS   \n",
       "1     F23  MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...   \n",
       "2     B60  PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...   \n",
       "3     D06  TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...   \n",
       "4     H04      ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE   \n",
       "\n",
       "                                                text  \n",
       "0  opc drum[sep]inorganic photoconductor drum[sep...  \n",
       "1  adjust gas flow[sep]altering gas flow[sep]mech...  \n",
       "2  lower trunnion[sep]lower locating[sep]performi...  \n",
       "3  cap component[sep]upper portion[sep]textiles; ...  \n",
       "4  neural stimulation[sep]artificial neural netwo...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = pd.read_csv(f\"{CFG_DEB_SIMPLE.input_path}test.csv\")\n",
    "\n",
    "# ====================================================\n",
    "# CPC Data\n",
    "# ====================================================\n",
    "cpc_texts = torch.load(\"../input/folds-dump-the-two-paths-fix/cpc_texts.pth\")\n",
    "test_df['context_text'] = test_df['context'].map(cpc_texts)\n",
    "test_df['text'] = test_df['anchor'] + '[SEP]' + test_df['target'] + '[SEP]'  + test_df['context_text']\n",
    "test_df['text'] = test_df['text'].apply(str.lower)\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d8612d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:00:54.133415Z",
     "iopub.status.busy": "2022-06-19T06:00:54.132630Z",
     "iopub.status.idle": "2022-06-19T06:00:55.300353Z",
     "shell.execute_reply": "2022-06-19T06:00:55.298894Z"
    },
    "papermill": {
     "duration": 1.367949,
     "end_time": "2022-06-19T06:00:55.302714",
     "exception": false,
     "start_time": "2022-06-19T06:00:53.934765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../input/deberta-v3-large/deberta-v3-large/config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"../input/deberta-v3-large/deberta-v3-large\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Didn't find file ../input/deberta-v3-large/deberta-v3-large/added_tokens.json. We won't load it.\n",
      "Didn't find file ../input/deberta-v3-large/deberta-v3-large/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ../input/deberta-v3-large/deberta-v3-large/tokenizer.json. We won't load it.\n",
      "loading file ../input/deberta-v3-large/deberta-v3-large/spm.model\n",
      "loading file None\n",
      "loading file None\n",
      "loading file ../input/deberta-v3-large/deberta-v3-large/tokenizer_config.json\n",
      "loading file None\n",
      "loading configuration file ../input/deberta-v3-large/deberta-v3-large/config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"../input/deberta-v3-large/deberta-v3-large\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Adding [MASK] to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_deberta_v3 = AutoTokenizer.from_pretrained(CFG_DEB_SIMPLE.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "07c45229",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:00:55.716123Z",
     "iopub.status.busy": "2022-06-19T06:00:55.715303Z",
     "iopub.status.idle": "2022-06-19T06:02:53.347124Z",
     "shell.execute_reply": "2022-06-19T06:02:53.345960Z"
    },
    "papermill": {
     "duration": 118.05841,
     "end_time": "2022-06-19T06:02:53.564634",
     "exception": false,
     "start_time": "2022-06-19T06:00:55.506224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]loading configuration file ../input/deberta-v3-large/deberta-v3-large/config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"../input/deberta-v3-large/deberta-v3-large\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      " 25%|██▌       | 1/4 [00:29<01:28, 29.38s/it]loading configuration file ../input/deberta-v3-large/deberta-v3-large/config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"../input/deberta-v3-large/deberta-v3-large\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      " 50%|█████     | 2/4 [00:58<00:58, 29.28s/it]loading configuration file ../input/deberta-v3-large/deberta-v3-large/config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"../input/deberta-v3-large/deberta-v3-large\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      " 75%|███████▌  | 3/4 [01:28<00:29, 29.41s/it]loading configuration file ../input/deberta-v3-large/deberta-v3-large/config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"../input/deberta-v3-large/deberta-v3-large\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "100%|██████████| 4/4 [01:57<00:00, 29.40s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "te_dataset = TestDataset(test_df, tokenizer_deberta_v3, CFG_DEB_SIMPLE.max_input_length)\n",
    "te_dataloader = DataLoader(te_dataset,\n",
    "                              batch_size=CFG_DEB_SIMPLE.batch_size, shuffle=False,\n",
    "                              num_workers=CFG_DEB_SIMPLE.num_workers,\n",
    "                              pin_memory=True, drop_last=False)\n",
    "\n",
    "deberta_simple_path = \"../input/us-patent-deberta-simple/microsoft_deberta-v3-large\"\n",
    "\n",
    "for fold in tqdm(range(CFG_DEB_SIMPLE.num_fold)):\n",
    "    fold_path = f\"{deberta_simple_path}_best{fold}.pth\"\n",
    "    \n",
    "    model = Custom_Bert_Simple(CFG_DEB_SIMPLE.model_path)\n",
    "    model.load_state_dict(torch.load(fold_path)['model'])\n",
    "    model.to('cuda')\n",
    "    \n",
    "    prediction = valid_fn(te_dataloader, model, 'cuda')\n",
    "    predictions.append(prediction)\n",
    "    \n",
    "    del model, prediction; gc.collect()\n",
    "    torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0338c7fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:02:53.982009Z",
     "iopub.status.busy": "2022-06-19T06:02:53.981008Z",
     "iopub.status.idle": "2022-06-19T06:02:53.990856Z",
     "shell.execute_reply": "2022-06-19T06:02:53.989841Z"
    },
    "papermill": {
     "duration": 0.222859,
     "end_time": "2022-06-19T06:02:53.993415",
     "exception": false,
     "start_time": "2022-06-19T06:02:53.770556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folds: 4\n",
      "rows:  36\n",
      "score: [0.5036294]\n"
     ]
    }
   ],
   "source": [
    "print(\"folds:\", len(predictions))\n",
    "print(\"rows: \", len(predictions[0]))\n",
    "print(\"score:\", predictions[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0a2d969a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:02:54.418686Z",
     "iopub.status.busy": "2022-06-19T06:02:54.417897Z",
     "iopub.status.idle": "2022-06-19T06:02:54.424457Z",
     "shell.execute_reply": "2022-06-19T06:02:54.423531Z"
    },
    "papermill": {
     "duration": 0.224373,
     "end_time": "2022-06-19T06:02:54.426620",
     "exception": false,
     "start_time": "2022-06-19T06:02:54.202247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_predictions = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b8f0f51d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:02:54.850751Z",
     "iopub.status.busy": "2022-06-19T06:02:54.850457Z",
     "iopub.status.idle": "2022-06-19T06:02:54.858423Z",
     "shell.execute_reply": "2022-06-19T06:02:54.857323Z"
    },
    "papermill": {
     "duration": 0.219877,
     "end_time": "2022-06-19T06:02:54.860731",
     "exception": false,
     "start_time": "2022-06-19T06:02:54.640854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5036294 ],\n",
       "       [ 0.4791839 ],\n",
       "       [ 0.49305084],\n",
       "       [ 0.2301636 ],\n",
       "       [ 0.4067986 ],\n",
       "       [ 0.5016197 ],\n",
       "       [ 0.516191  ],\n",
       "       [-0.02492346],\n",
       "       [ 0.1932471 ],\n",
       "       [ 0.93988395],\n",
       "       [ 0.2218402 ],\n",
       "       [ 0.27013338],\n",
       "       [ 0.7560612 ],\n",
       "       [ 0.832283  ]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first fold\n",
    "predictions[0][:n_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6e35a284",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:02:55.279862Z",
     "iopub.status.busy": "2022-06-19T06:02:55.279585Z",
     "iopub.status.idle": "2022-06-19T06:02:55.287807Z",
     "shell.execute_reply": "2022-06-19T06:02:55.286816Z"
    },
    "papermill": {
     "duration": 0.219857,
     "end_time": "2022-06-19T06:02:55.290253",
     "exception": false,
     "start_time": "2022-06-19T06:02:55.070396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# np.where(x<=0, 0, x) .. >> min_max.fit_transform(x) >> x.reshape(-1)\n",
    "upd_predictions = [upd_outputs(x, is_trim=False) for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dfa426d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:02:55.986611Z",
     "iopub.status.busy": "2022-06-19T06:02:55.986151Z",
     "iopub.status.idle": "2022-06-19T06:02:56.000433Z",
     "shell.execute_reply": "2022-06-19T06:02:55.999032Z"
    },
    "papermill": {
     "duration": 0.382624,
     "end_time": "2022-06-19T06:02:56.003145",
     "exception": false,
     "start_time": "2022-06-19T06:02:55.620521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === add np.median ===\n",
    "add_preds = []\n",
    "for x in zip(*upd_predictions):\n",
    "    add_preds.append(np.median(x, axis=0))\n",
    "    \n",
    "upd_predictions.append(add_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "939ed053",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:02:56.704046Z",
     "iopub.status.busy": "2022-06-19T06:02:56.703620Z",
     "iopub.status.idle": "2022-06-19T06:02:56.714407Z",
     "shell.execute_reply": "2022-06-19T06:02:56.713441Z"
    },
    "papermill": {
     "duration": 0.447379,
     "end_time": "2022-06-19T06:02:56.717654",
     "exception": false,
     "start_time": "2022-06-19T06:02:56.270275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === add np.mean ===\n",
    "add_preds = []\n",
    "for x in zip(*upd_predictions):\n",
    "    add_preds.append(np.mean(x, axis=0))\n",
    "    \n",
    "upd_predictions.append(add_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8b921eda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:02:57.432842Z",
     "iopub.status.busy": "2022-06-19T06:02:57.432470Z",
     "iopub.status.idle": "2022-06-19T06:02:57.437823Z",
     "shell.execute_reply": "2022-06-19T06:02:57.436918Z"
    },
    "papermill": {
     "duration": 0.355998,
     "end_time": "2022-06-19T06:02:57.442852",
     "exception": false,
     "start_time": "2022-06-19T06:02:57.086854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred5 = np.mean(upd_predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8e3cc17d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:02:58.044930Z",
     "iopub.status.busy": "2022-06-19T06:02:58.044613Z",
     "iopub.status.idle": "2022-06-19T06:02:58.053938Z",
     "shell.execute_reply": "2022-06-19T06:02:58.052498Z"
    },
    "papermill": {
     "duration": 0.269765,
     "end_time": "2022-06-19T06:02:58.056506",
     "exception": false,
     "start_time": "2022-06-19T06:02:57.786741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31270644 0.59676236 0.5130505 0.28154138 0.09131227 0.4976574 0.46910086 0.0035888886 0.25367817 0.9989567 0.25185874 0.2716439 0.77297974 0.8849662\n"
     ]
    }
   ],
   "source": [
    "print(*pred5[:n_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7432ced6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:02:58.481946Z",
     "iopub.status.busy": "2022-06-19T06:02:58.481654Z",
     "iopub.status.idle": "2022-06-19T06:02:58.495599Z",
     "shell.execute_reply": "2022-06-19T06:02:58.494565Z"
    },
    "papermill": {
     "duration": 0.230113,
     "end_time": "2022-06-19T06:02:58.497942",
     "exception": false,
     "start_time": "2022-06-19T06:02:58.267829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>0.312706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09e418c93a776564</td>\n",
       "      <td>0.596762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36baf228038e314b</td>\n",
       "      <td>0.513050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f37ead645e7f0c8</td>\n",
       "      <td>0.281541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71a5b6ad068d531f</td>\n",
       "      <td>0.091312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>474c874d0c07bd21</td>\n",
       "      <td>0.497657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>442c114ed5c4e3c9</td>\n",
       "      <td>0.469101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b8ae62ea5e1d8bdb</td>\n",
       "      <td>0.003589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>faaddaf8fcba8a3f</td>\n",
       "      <td>0.253678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ae0262c02566d2ce</td>\n",
       "      <td>0.998957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a8808e31641e856d</td>\n",
       "      <td>0.251859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16ae4b99d3601e60</td>\n",
       "      <td>0.271644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25c555ca3d5a2092</td>\n",
       "      <td>0.772980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5203a36c501f1b7c</td>\n",
       "      <td>0.884966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     score\n",
       "0   4112d61851461f60  0.312706\n",
       "1   09e418c93a776564  0.596762\n",
       "2   36baf228038e314b  0.513050\n",
       "3   1f37ead645e7f0c8  0.281541\n",
       "4   71a5b6ad068d531f  0.091312\n",
       "5   474c874d0c07bd21  0.497657\n",
       "6   442c114ed5c4e3c9  0.469101\n",
       "7   b8ae62ea5e1d8bdb  0.003589\n",
       "8   faaddaf8fcba8a3f  0.253678\n",
       "9   ae0262c02566d2ce  0.998957\n",
       "10  a8808e31641e856d  0.251859\n",
       "11  16ae4b99d3601e60  0.271644\n",
       "12  25c555ca3d5a2092  0.772980\n",
       "13  5203a36c501f1b7c  0.884966"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_5 = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'score': pred5,\n",
    "})\n",
    "\n",
    "submission_5.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8470e9ca",
   "metadata": {
    "papermill": {
     "duration": 0.210666,
     "end_time": "2022-06-19T06:02:58.917600",
     "exception": false,
     "start_time": "2022-06-19T06:02:58.706934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6.electra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e3e8a378",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:02:59.348449Z",
     "iopub.status.busy": "2022-06-19T06:02:59.348135Z",
     "iopub.status.idle": "2022-06-19T06:02:59.353557Z",
     "shell.execute_reply": "2022-06-19T06:02:59.352336Z"
    },
    "papermill": {
     "duration": 0.226348,
     "end_time": "2022-06-19T06:02:59.356289",
     "exception": false,
     "start_time": "2022-06-19T06:02:59.129941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "INPUT_DIR = '../input/us-patent-phrase-to-phrase-matching/'\n",
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c269dcd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:02:59.778737Z",
     "iopub.status.busy": "2022-06-19T06:02:59.778479Z",
     "iopub.status.idle": "2022-06-19T06:02:59.858407Z",
     "shell.execute_reply": "2022-06-19T06:02:59.857343Z"
    },
    "papermill": {
     "duration": 0.295572,
     "end_time": "2022-06-19T06:02:59.860870",
     "exception": false,
     "start_time": "2022-06-19T06:02:59.565298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ../input/electrav1/tokenizer/tokenizer/added_tokens.json. We won't load it.\n",
      "loading file ../input/electrav1/tokenizer/tokenizer/vocab.txt\n",
      "loading file ../input/electrav1/tokenizer/tokenizer/tokenizer.json\n",
      "loading file None\n",
      "loading file ../input/electrav1/tokenizer/tokenizer/special_tokens_map.json\n",
      "loading file ../input/electrav1/tokenizer/tokenizer/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    num_workers=4\n",
    "    path=\"../input/electrav1/\"\n",
    "    config_path=path+'config.pth'\n",
    "    batch_size=32\n",
    "    fc_dropout=0.2\n",
    "    target_size=1\n",
    "    max_len=133\n",
    "    seed=42\n",
    "    n_fold=4\n",
    "    trn_fold=[0, 1, 2, 3]\n",
    "    tokenizer = AutoTokenizer.from_pretrained('../input/electrav1/tokenizer/tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3cdab6bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:03:00.292771Z",
     "iopub.status.busy": "2022-06-19T06:03:00.292467Z",
     "iopub.status.idle": "2022-06-19T06:03:00.303493Z",
     "shell.execute_reply": "2022-06-19T06:03:00.302270Z"
    },
    "papermill": {
     "duration": 0.231448,
     "end_time": "2022-06-19T06:03:00.306070",
     "exception": false,
     "start_time": "2022-06-19T06:03:00.074622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_true, y_pred):\n",
    "    score = sp.stats.pearsonr(y_true, y_pred)[0]\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c984a393",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:03:00.738991Z",
     "iopub.status.busy": "2022-06-19T06:03:00.738240Z",
     "iopub.status.idle": "2022-06-19T06:03:00.757447Z",
     "shell.execute_reply": "2022-06-19T06:03:00.756334Z"
    },
    "papermill": {
     "duration": 0.235918,
     "end_time": "2022-06-19T06:03:00.759732",
     "exception": false,
     "start_time": "2022-06-19T06:03:00.523814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.shape: (36, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>opc drum</td>\n",
       "      <td>inorganic photoconductor drum</td>\n",
       "      <td>G02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09e418c93a776564</td>\n",
       "      <td>adjust gas flow</td>\n",
       "      <td>altering gas flow</td>\n",
       "      <td>F23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36baf228038e314b</td>\n",
       "      <td>lower trunnion</td>\n",
       "      <td>lower locating</td>\n",
       "      <td>B60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f37ead645e7f0c8</td>\n",
       "      <td>cap component</td>\n",
       "      <td>upper portion</td>\n",
       "      <td>D06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71a5b6ad068d531f</td>\n",
       "      <td>neural stimulation</td>\n",
       "      <td>artificial neural network</td>\n",
       "      <td>H04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id              anchor                         target context\n",
       "0  4112d61851461f60            opc drum  inorganic photoconductor drum     G02\n",
       "1  09e418c93a776564     adjust gas flow              altering gas flow     F23\n",
       "2  36baf228038e314b      lower trunnion                 lower locating     B60\n",
       "3  1f37ead645e7f0c8       cap component                  upper portion     D06\n",
       "4  71a5b6ad068d531f  neural stimulation      artificial neural network     H04"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "test = pd.read_csv(INPUT_DIR+'test.csv')\n",
    "print(f\"test.shape: {test.shape}\")\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c73141d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:03:01.185478Z",
     "iopub.status.busy": "2022-06-19T06:03:01.184599Z",
     "iopub.status.idle": "2022-06-19T06:03:01.204366Z",
     "shell.execute_reply": "2022-06-19T06:03:01.203272Z"
    },
    "papermill": {
     "duration": 0.233382,
     "end_time": "2022-06-19T06:03:01.206482",
     "exception": false,
     "start_time": "2022-06-19T06:03:00.973100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>context_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>opc drum</td>\n",
       "      <td>inorganic photoconductor drum</td>\n",
       "      <td>G02</td>\n",
       "      <td>PHYSICS. OPTICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09e418c93a776564</td>\n",
       "      <td>adjust gas flow</td>\n",
       "      <td>altering gas flow</td>\n",
       "      <td>F23</td>\n",
       "      <td>MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36baf228038e314b</td>\n",
       "      <td>lower trunnion</td>\n",
       "      <td>lower locating</td>\n",
       "      <td>B60</td>\n",
       "      <td>PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f37ead645e7f0c8</td>\n",
       "      <td>cap component</td>\n",
       "      <td>upper portion</td>\n",
       "      <td>D06</td>\n",
       "      <td>TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71a5b6ad068d531f</td>\n",
       "      <td>neural stimulation</td>\n",
       "      <td>artificial neural network</td>\n",
       "      <td>H04</td>\n",
       "      <td>ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id              anchor                         target  \\\n",
       "0  4112d61851461f60            opc drum  inorganic photoconductor drum   \n",
       "1  09e418c93a776564     adjust gas flow              altering gas flow   \n",
       "2  36baf228038e314b      lower trunnion                 lower locating   \n",
       "3  1f37ead645e7f0c8       cap component                  upper portion   \n",
       "4  71a5b6ad068d531f  neural stimulation      artificial neural network   \n",
       "\n",
       "  context                                       context_text  \n",
       "0     G02                                    PHYSICS. OPTICS  \n",
       "1     F23  MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...  \n",
       "2     B60  PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...  \n",
       "3     D06  TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...  \n",
       "4     H04      ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# CPC Data\n",
    "# ====================================================\n",
    "cpc_texts = torch.load(\"../input/pppm-deberta-v3-large-baseline-w-w-b-train/cpc_texts.pth\")\n",
    "test['context_text'] = test['context'].map(cpc_texts)\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "47b035e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:03:01.649033Z",
     "iopub.status.busy": "2022-06-19T06:03:01.648735Z",
     "iopub.status.idle": "2022-06-19T06:03:01.664876Z",
     "shell.execute_reply": "2022-06-19T06:03:01.663942Z"
    },
    "papermill": {
     "duration": 0.236604,
     "end_time": "2022-06-19T06:03:01.667380",
     "exception": false,
     "start_time": "2022-06-19T06:03:01.430776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>context_text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>opc drum</td>\n",
       "      <td>inorganic photoconductor drum</td>\n",
       "      <td>G02</td>\n",
       "      <td>PHYSICS. OPTICS</td>\n",
       "      <td>opc drum[SEP]inorganic photoconductor drum[SEP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09e418c93a776564</td>\n",
       "      <td>adjust gas flow</td>\n",
       "      <td>altering gas flow</td>\n",
       "      <td>F23</td>\n",
       "      <td>MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...</td>\n",
       "      <td>adjust gas flow[SEP]altering gas flow[SEP]MECH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36baf228038e314b</td>\n",
       "      <td>lower trunnion</td>\n",
       "      <td>lower locating</td>\n",
       "      <td>B60</td>\n",
       "      <td>PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...</td>\n",
       "      <td>lower trunnion[SEP]lower locating[SEP]PERFORMI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f37ead645e7f0c8</td>\n",
       "      <td>cap component</td>\n",
       "      <td>upper portion</td>\n",
       "      <td>D06</td>\n",
       "      <td>TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...</td>\n",
       "      <td>cap component[SEP]upper portion[SEP]TEXTILES; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71a5b6ad068d531f</td>\n",
       "      <td>neural stimulation</td>\n",
       "      <td>artificial neural network</td>\n",
       "      <td>H04</td>\n",
       "      <td>ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE</td>\n",
       "      <td>neural stimulation[SEP]artificial neural netwo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id              anchor                         target  \\\n",
       "0  4112d61851461f60            opc drum  inorganic photoconductor drum   \n",
       "1  09e418c93a776564     adjust gas flow              altering gas flow   \n",
       "2  36baf228038e314b      lower trunnion                 lower locating   \n",
       "3  1f37ead645e7f0c8       cap component                  upper portion   \n",
       "4  71a5b6ad068d531f  neural stimulation      artificial neural network   \n",
       "\n",
       "  context                                       context_text  \\\n",
       "0     G02                                    PHYSICS. OPTICS   \n",
       "1     F23  MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...   \n",
       "2     B60  PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...   \n",
       "3     D06  TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...   \n",
       "4     H04      ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE   \n",
       "\n",
       "                                                text  \n",
       "0  opc drum[SEP]inorganic photoconductor drum[SEP...  \n",
       "1  adjust gas flow[SEP]altering gas flow[SEP]MECH...  \n",
       "2  lower trunnion[SEP]lower locating[SEP]PERFORMI...  \n",
       "3  cap component[SEP]upper portion[SEP]TEXTILES; ...  \n",
       "4  neural stimulation[SEP]artificial neural netwo...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0d713f69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:03:02.100149Z",
     "iopub.status.busy": "2022-06-19T06:03:02.099353Z",
     "iopub.status.idle": "2022-06-19T06:03:02.107681Z",
     "shell.execute_reply": "2022-06-19T06:03:02.106670Z"
    },
    "papermill": {
     "duration": 0.2251,
     "end_time": "2022-06-19T06:03:02.109822",
     "exception": false,
     "start_time": "2022-06-19T06:03:01.884722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(cfg, text):\n",
    "    inputs = cfg.tokenizer(text,\n",
    "                           add_special_tokens=True,\n",
    "                           max_length=cfg.max_len,\n",
    "                           padding=\"max_length\",\n",
    "                           return_offsets_mapping=False)\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['text'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, self.texts[item])\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3f4c2515",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:03:02.541131Z",
     "iopub.status.busy": "2022-06-19T06:03:02.540772Z",
     "iopub.status.idle": "2022-06-19T06:03:02.558497Z",
     "shell.execute_reply": "2022-06-19T06:03:02.557549Z"
    },
    "papermill": {
     "duration": 0.234664,
     "end_time": "2022-06-19T06:03:02.560674",
     "exception": false,
     "start_time": "2022-06-19T06:03:02.326010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel.from_config(self.config)\n",
    "        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n",
    "        self._init_weights(self.fc)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.config.hidden_size, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(self.config.hidden_size)\n",
    "        self._init_weights(self.attention)\n",
    "        self.linear = nn.Linear(self.config.hidden_size, 1)\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        # feature = torch.mean(last_hidden_states, 1)\n",
    "        weights = self.attention(last_hidden_states)\n",
    "        feature = torch.sum(weights * last_hidden_states, dim=1)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_state = outputs[0]\n",
    "        input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        out = sum_embeddings / sum_mask\n",
    "        \n",
    "        out = self.layer_norm1(out)\n",
    "        output = self.fc(out)\n",
    "        \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4936b89f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:03:02.986189Z",
     "iopub.status.busy": "2022-06-19T06:03:02.985865Z",
     "iopub.status.idle": "2022-06-19T06:03:02.993224Z",
     "shell.execute_reply": "2022-06-19T06:03:02.992243Z"
    },
    "papermill": {
     "duration": 0.221784,
     "end_time": "2022-06-19T06:03:02.995536",
     "exception": false,
     "start_time": "2022-06-19T06:03:02.773752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# inference\n",
    "# ====================================================\n",
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5c4eb2b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:03:03.428374Z",
     "iopub.status.busy": "2022-06-19T06:03:03.427981Z",
     "iopub.status.idle": "2022-06-19T06:04:27.920813Z",
     "shell.execute_reply": "2022-06-19T06:04:27.919421Z"
    },
    "papermill": {
     "duration": 84.7118,
     "end_time": "2022-06-19T06:04:27.923209",
     "exception": false,
     "start_time": "2022-06-19T06:03:03.211409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.82it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.53it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.82it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TestDataset(CFG, test)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=CFG.batch_size,\n",
    "                         shuffle=False,\n",
    "                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "predictions = []\n",
    "for fold in range(4):\n",
    "    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n",
    "    state = torch.load(f'../input/electrav1/google-electra-large-discriminator_fold{fold}_best/google-electra-large-discriminator_fold{fold}_best.pth',\n",
    "                       map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state['model'])\n",
    "    prediction = inference_fn(test_loader, model, device)\n",
    "    predictions.append(prediction)\n",
    "    del model, state, prediction; gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "pred7 = np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "23c22d24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:04:28.375729Z",
     "iopub.status.busy": "2022-06-19T06:04:28.375420Z",
     "iopub.status.idle": "2022-06-19T06:04:28.381749Z",
     "shell.execute_reply": "2022-06-19T06:04:28.380479Z"
    },
    "papermill": {
     "duration": 0.238813,
     "end_time": "2022-06-19T06:04:28.384831",
     "exception": false,
     "start_time": "2022-06-19T06:04:28.146018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MMscaler = MinMaxScaler()\n",
    "pred7_mm = MMscaler.fit_transform(pred7.reshape(-1,1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "64b55f5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:04:28.834638Z",
     "iopub.status.busy": "2022-06-19T06:04:28.834348Z",
     "iopub.status.idle": "2022-06-19T06:04:28.850208Z",
     "shell.execute_reply": "2022-06-19T06:04:28.849118Z"
    },
    "papermill": {
     "duration": 0.243555,
     "end_time": "2022-06-19T06:04:28.852632",
     "exception": false,
     "start_time": "2022-06-19T06:04:28.609077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>0.609093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09e418c93a776564</td>\n",
       "      <td>0.726084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36baf228038e314b</td>\n",
       "      <td>0.326839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f37ead645e7f0c8</td>\n",
       "      <td>0.323469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71a5b6ad068d531f</td>\n",
       "      <td>0.000843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>474c874d0c07bd21</td>\n",
       "      <td>0.522739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>442c114ed5c4e3c9</td>\n",
       "      <td>0.379698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b8ae62ea5e1d8bdb</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>faaddaf8fcba8a3f</td>\n",
       "      <td>0.333683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ae0262c02566d2ce</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a8808e31641e856d</td>\n",
       "      <td>0.235588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16ae4b99d3601e60</td>\n",
       "      <td>0.204363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25c555ca3d5a2092</td>\n",
       "      <td>0.770692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5203a36c501f1b7c</td>\n",
       "      <td>0.757917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     score\n",
       "0   4112d61851461f60  0.609093\n",
       "1   09e418c93a776564  0.726084\n",
       "2   36baf228038e314b  0.326839\n",
       "3   1f37ead645e7f0c8  0.323469\n",
       "4   71a5b6ad068d531f  0.000843\n",
       "5   474c874d0c07bd21  0.522739\n",
       "6   442c114ed5c4e3c9  0.379698\n",
       "7   b8ae62ea5e1d8bdb  0.000000\n",
       "8   faaddaf8fcba8a3f  0.333683\n",
       "9   ae0262c02566d2ce  1.000000\n",
       "10  a8808e31641e856d  0.235588\n",
       "11  16ae4b99d3601e60  0.204363\n",
       "12  25c555ca3d5a2092  0.770692\n",
       "13  5203a36c501f1b7c  0.757917"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_7 = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'score': pred7_mm,\n",
    "})\n",
    "\n",
    "submission_7.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "18bf1e6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:04:29.303992Z",
     "iopub.status.busy": "2022-06-19T06:04:29.303694Z",
     "iopub.status.idle": "2022-06-19T06:04:29.374683Z",
     "shell.execute_reply": "2022-06-19T06:04:29.373623Z"
    },
    "papermill": {
     "duration": 0.300303,
     "end_time": "2022-06-19T06:04:29.377346",
     "exception": false,
     "start_time": "2022-06-19T06:04:29.077043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ../input/google-electra-large-discriminator/added_tokens.json. We won't load it.\n",
      "loading file ../input/google-electra-large-discriminator/vocab.txt\n",
      "loading file ../input/google-electra-large-discriminator/tokenizer.json\n",
      "loading file None\n",
      "loading file ../input/google-electra-large-discriminator/special_tokens_map.json\n",
      "loading file ../input/google-electra-large-discriminator/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    num_workers=4\n",
    "    path=\"../input/electrav5/\"\n",
    "    config_path=path+'config.pth'\n",
    "    batch_size=32\n",
    "    fc_dropout=0.1\n",
    "    target_size=1\n",
    "    max_len=133\n",
    "    seed=34\n",
    "    n_fold=4\n",
    "    trn_fold=[0, 1, 2, 3]\n",
    "    tokenizer = AutoTokenizer.from_pretrained('../input/google-electra-large-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cdf08354",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:04:29.833261Z",
     "iopub.status.busy": "2022-06-19T06:04:29.832614Z",
     "iopub.status.idle": "2022-06-19T06:06:03.112807Z",
     "shell.execute_reply": "2022-06-19T06:06:03.111603Z"
    },
    "papermill": {
     "duration": 93.509557,
     "end_time": "2022-06-19T06:06:03.116358",
     "exception": false,
     "start_time": "2022-06-19T06:04:29.606801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.63it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.57it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.69it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TestDataset(CFG, test)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=CFG.batch_size,\n",
    "                         shuffle=False,\n",
    "                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "predictions = []\n",
    "for fold in range(4):\n",
    "    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n",
    "    state = torch.load(f'../input/electrav5/google-electra-large-discriminator_fold{fold}_best.pth',\n",
    "                       map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state['model'])\n",
    "    prediction = inference_fn(test_loader, model, device)\n",
    "    predictions.append(prediction)\n",
    "    del model, state, prediction; gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "pred8 = np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3eb549e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:06:03.635589Z",
     "iopub.status.busy": "2022-06-19T06:06:03.635203Z",
     "iopub.status.idle": "2022-06-19T06:06:03.641840Z",
     "shell.execute_reply": "2022-06-19T06:06:03.640811Z"
    },
    "papermill": {
     "duration": 0.243988,
     "end_time": "2022-06-19T06:06:03.644178",
     "exception": false,
     "start_time": "2022-06-19T06:06:03.400190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MMscaler = MinMaxScaler()\n",
    "pred8_mm = MMscaler.fit_transform(pred8.reshape(-1,1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "27569d66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:06:04.113993Z",
     "iopub.status.busy": "2022-06-19T06:06:04.113700Z",
     "iopub.status.idle": "2022-06-19T06:06:04.126941Z",
     "shell.execute_reply": "2022-06-19T06:06:04.125904Z"
    },
    "papermill": {
     "duration": 0.251113,
     "end_time": "2022-06-19T06:06:04.129595",
     "exception": false,
     "start_time": "2022-06-19T06:06:03.878482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>0.599164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09e418c93a776564</td>\n",
       "      <td>0.794765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36baf228038e314b</td>\n",
       "      <td>0.342252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f37ead645e7f0c8</td>\n",
       "      <td>0.334473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71a5b6ad068d531f</td>\n",
       "      <td>0.072055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>474c874d0c07bd21</td>\n",
       "      <td>0.543924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>442c114ed5c4e3c9</td>\n",
       "      <td>0.340275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b8ae62ea5e1d8bdb</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>faaddaf8fcba8a3f</td>\n",
       "      <td>0.367220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ae0262c02566d2ce</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a8808e31641e856d</td>\n",
       "      <td>0.216157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16ae4b99d3601e60</td>\n",
       "      <td>0.218741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25c555ca3d5a2092</td>\n",
       "      <td>0.765401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5203a36c501f1b7c</td>\n",
       "      <td>0.777588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     score\n",
       "0   4112d61851461f60  0.599164\n",
       "1   09e418c93a776564  0.794765\n",
       "2   36baf228038e314b  0.342252\n",
       "3   1f37ead645e7f0c8  0.334473\n",
       "4   71a5b6ad068d531f  0.072055\n",
       "5   474c874d0c07bd21  0.543924\n",
       "6   442c114ed5c4e3c9  0.340275\n",
       "7   b8ae62ea5e1d8bdb  0.000024\n",
       "8   faaddaf8fcba8a3f  0.367220\n",
       "9   ae0262c02566d2ce  1.000000\n",
       "10  a8808e31641e856d  0.216157\n",
       "11  16ae4b99d3601e60  0.218741\n",
       "12  25c555ca3d5a2092  0.765401\n",
       "13  5203a36c501f1b7c  0.777588"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_8 = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'score': pred8_mm,\n",
    "})\n",
    "\n",
    "submission_8.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb89dba6",
   "metadata": {
    "papermill": {
     "duration": 0.236971,
     "end_time": "2022-06-19T06:06:04.605452",
     "exception": false,
     "start_time": "2022-06-19T06:06:04.368481",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7.xlm roberta large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0672823c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:06:05.077417Z",
     "iopub.status.busy": "2022-06-19T06:06:05.077065Z",
     "iopub.status.idle": "2022-06-19T06:06:05.082281Z",
     "shell.execute_reply": "2022-06-19T06:06:05.081291Z"
    },
    "papermill": {
     "duration": 0.243363,
     "end_time": "2022-06-19T06:06:05.084501",
     "exception": false,
     "start_time": "2022-06-19T06:06:04.841138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    input_path = '../input/us-patent-phrase-to-phrase-matching/'\n",
    "    model_path = '../input/xlm-roberta-large-5folds/',\n",
    "    model_num = 1\n",
    "    num_fold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ab31bf46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:06:05.558401Z",
     "iopub.status.busy": "2022-06-19T06:06:05.558066Z",
     "iopub.status.idle": "2022-06-19T06:06:06.476735Z",
     "shell.execute_reply": "2022-06-19T06:06:06.475720Z"
    },
    "papermill": {
     "duration": 1.158521,
     "end_time": "2022-06-19T06:06:06.479285",
     "exception": false,
     "start_time": "2022-06-19T06:06:05.320764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "titles = pd.read_csv('../input/cpc-codes/titles.csv')\n",
    "test = pd.read_csv(f\"{CFG.input_path}test.csv\")\n",
    "test.reset_index(inplace=True)\n",
    "test = test.merge(titles, left_on='context', right_on='code')\n",
    "test.sort_values(by='index', inplace=True)\n",
    "test.drop(columns='index', inplace=True)\n",
    "test.reset_index(inplace=True)\n",
    "test['input'] = test['title']+'[SEP]'+test['anchor']\n",
    "test = test.drop(columns=[\"context\", \"code\", \"class\", \"subclass\", \"group\", \"main_group\", \"anchor\", \"title\", \"section\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "52390f27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:06:06.953524Z",
     "iopub.status.busy": "2022-06-19T06:06:06.953195Z",
     "iopub.status.idle": "2022-06-19T06:06:06.961332Z",
     "shell.execute_reply": "2022-06-19T06:06:06.960036Z"
    },
    "papermill": {
     "duration": 0.249172,
     "end_time": "2022-06-19T06:06:06.963848",
     "exception": false,
     "start_time": "2022-06-19T06:06:06.714676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InferDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.inputs = df['input'].values.astype(str)\n",
    "        self.targets = df['target'].values.astype(str)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = self.inputs[item]\n",
    "        targets = self.targets[item]\n",
    "        \n",
    "        return {\n",
    "        **tokenizer( inputs, targets )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7b90b90f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:06:07.436990Z",
     "iopub.status.busy": "2022-06-19T06:06:07.436672Z",
     "iopub.status.idle": "2022-06-19T06:08:26.541074Z",
     "shell.execute_reply": "2022-06-19T06:08:26.539995Z"
    },
    "papermill": {
     "duration": 139.344345,
     "end_time": "2022-06-19T06:08:26.543877",
     "exception": false,
     "start_time": "2022-06-19T06:06:07.199532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ../input/xlm-roberta-large-5folds/fold0/added_tokens.json. We won't load it.\n",
      "loading file ../input/xlm-roberta-large-5folds/fold0/sentencepiece.bpe.model\n",
      "loading file ../input/xlm-roberta-large-5folds/fold0/tokenizer.json\n",
      "loading file None\n",
      "loading file ../input/xlm-roberta-large-5folds/fold0/special_tokens_map.json\n",
      "loading file ../input/xlm-roberta-large-5folds/fold0/tokenizer_config.json\n",
      "loading configuration file ../input/xlm-roberta-large-5folds/fold0/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"../input/xlm-roberta-large-5folds/fold0\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file ../input/xlm-roberta-large-5folds/fold0/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at ../input/xlm-roberta-large-5folds/fold0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../input/xlm-roberta-large-5folds/fold1/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"../input/xlm-roberta-large-5folds/fold1\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file ../input/xlm-roberta-large-5folds/fold1/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at ../input/xlm-roberta-large-5folds/fold1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../input/xlm-roberta-large-5folds/fold2/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"../input/xlm-roberta-large-5folds/fold2\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file ../input/xlm-roberta-large-5folds/fold2/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at ../input/xlm-roberta-large-5folds/fold2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../input/xlm-roberta-large-5folds/fold3/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"../input/xlm-roberta-large-5folds/fold3\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file ../input/xlm-roberta-large-5folds/fold3/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at ../input/xlm-roberta-large-5folds/fold3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../input/xlm-roberta-large-5folds/fold4/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"../input/xlm-roberta-large-5folds/fold4\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file ../input/xlm-roberta-large-5folds/fold4/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at ../input/xlm-roberta-large-5folds/fold4.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = []\n",
    "MMscaler = MinMaxScaler()\n",
    "\n",
    "for i in range (CFG.model_num):   \n",
    "    tokenizer = AutoTokenizer.from_pretrained(f'{CFG.model_path[i]}fold0')\n",
    "\n",
    "    def process_test(unit):\n",
    "            return {\n",
    "            **tokenizer( unit['input'], unit['target'])\n",
    "        }\n",
    "    \n",
    "    def process_valid(unit):\n",
    "        return {\n",
    "        **tokenizer( unit['input'], unit['target']),\n",
    "        'label': unit['score']\n",
    "    }\n",
    "    \n",
    "    test_ds = InferDataset(test)\n",
    "    \n",
    "    for fold in range(CFG.num_fold):        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(f'{CFG.model_path[i]}fold{fold}', \n",
    "                                                                   num_labels=1)\n",
    "        trainer = Trainer(\n",
    "                model,\n",
    "                tokenizer=tokenizer,\n",
    "            )\n",
    "        \n",
    "        outputs = trainer.predict(test_ds)\n",
    "        prediction = MMscaler.fit_transform(outputs.predictions.reshape(-1, 1)).reshape(-1)\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "        del model, prediction; gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "pred9_mm = np.mean(predictions, axis=0)\n",
    "# np.clip(predictions, 0.0, 1.0, out=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8ccceadc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:08:27.068691Z",
     "iopub.status.busy": "2022-06-19T06:08:27.068392Z",
     "iopub.status.idle": "2022-06-19T06:08:27.074759Z",
     "shell.execute_reply": "2022-06-19T06:08:27.073766Z"
    },
    "papermill": {
     "duration": 0.271558,
     "end_time": "2022-06-19T06:08:27.077375",
     "exception": false,
     "start_time": "2022-06-19T06:08:26.805817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_9 = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'score': pred9_mm,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2615da7d",
   "metadata": {
    "papermill": {
     "duration": 0.438661,
     "end_time": "2022-06-19T06:08:27.776224",
     "exception": false,
     "start_time": "2022-06-19T06:08:27.337563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# post-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2914df85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:08:28.666904Z",
     "iopub.status.busy": "2022-06-19T06:08:28.665926Z",
     "iopub.status.idle": "2022-06-19T06:08:28.801254Z",
     "shell.execute_reply": "2022-06-19T06:08:28.800307Z"
    },
    "papermill": {
     "duration": 0.574801,
     "end_time": "2022-06-19T06:08:28.804689",
     "exception": false,
     "start_time": "2022-06-19T06:08:28.229888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "thresholds_dict = {\n",
    "    '0': 0.02,\n",
    "    '.25': (0.24, 0.26),\n",
    "    '.50': (0.49, 0.51),\n",
    "    '.75': (0.74, 0.76),\n",
    "    '1': 0.98\n",
    "}\n",
    "\n",
    "# No.1\n",
    "submission_1['score'] = upd_score(submission_1['score'], thresholds_dict)\n",
    "# No.2\n",
    "submission_2['score'] = upd_score(submission_2['score'], thresholds_dict)\n",
    "# No.3\n",
    "submission_3['score'] = upd_score(submission_3['score'], thresholds_dict)\n",
    "# No.4\n",
    "submission_4['score'] = upd_score(submission_4['score'], thresholds_dict)\n",
    "# No.5\n",
    "submission_5['score'] = upd_score(submission_5['score'], thresholds_dict)\n",
    "# No.6\n",
    "submission_6['score'] = upd_score(submission_6['score'], thresholds_dict)\n",
    "# No.7\n",
    "submission_7['score'] = upd_score(submission_7['score'], thresholds_dict)\n",
    "# No.8\n",
    "submission_8['score'] = upd_score(submission_8['score'], thresholds_dict)\n",
    "# No.9\n",
    "submission_9['score'] = upd_score(submission_9['score'], thresholds_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e32791",
   "metadata": {
    "papermill": {
     "duration": 0.26128,
     "end_time": "2022-06-19T06:08:29.420220",
     "exception": false,
     "start_time": "2022-06-19T06:08:29.158940",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2d0601b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:08:29.934305Z",
     "iopub.status.busy": "2022-06-19T06:08:29.933988Z",
     "iopub.status.idle": "2022-06-19T06:08:29.939392Z",
     "shell.execute_reply": "2022-06-19T06:08:29.938420Z"
    },
    "papermill": {
     "duration": 0.261986,
     "end_time": "2022-06-19T06:08:29.941527",
     "exception": false,
     "start_time": "2022-06-19T06:08:29.679541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "w1 = 0.14\n",
    "w2 = 0.07\n",
    "w3 = 0.11\n",
    "w4 = 0.05\n",
    "w5 = 0.28\n",
    "w6 = 0.10\n",
    "w7 = 0.12\n",
    "w8 = 0.08\n",
    "w9 = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a910658d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:08:30.460971Z",
     "iopub.status.busy": "2022-06-19T06:08:30.460660Z",
     "iopub.status.idle": "2022-06-19T06:08:30.472992Z",
     "shell.execute_reply": "2022-06-19T06:08:30.471132Z"
    },
    "papermill": {
     "duration": 0.276848,
     "end_time": "2022-06-19T06:08:30.475287",
     "exception": false,
     "start_time": "2022-06-19T06:08:30.198439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_1['score'] = submission_1['score'] * w1 + submission_2['score'] * w2 + submission_3['score'] * w3 + submission_4['score'] * w4 + submission_5['score'] * w5 + submission_6['score'] * w6 + submission_7['score'] * w7 + submission_8['score'] * w8 + submission_9['score'] * w9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2436aea1",
   "metadata": {
    "papermill": {
     "duration": 0.260682,
     "end_time": "2022-06-19T06:08:31.002076",
     "exception": false,
     "start_time": "2022-06-19T06:08:30.741394",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "31b4256d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:08:31.542344Z",
     "iopub.status.busy": "2022-06-19T06:08:31.542020Z",
     "iopub.status.idle": "2022-06-19T06:08:31.552790Z",
     "shell.execute_reply": "2022-06-19T06:08:31.551809Z"
    },
    "papermill": {
     "duration": 0.294406,
     "end_time": "2022-06-19T06:08:31.554939",
     "exception": false,
     "start_time": "2022-06-19T06:08:31.260533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>0.462946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09e418c93a776564</td>\n",
       "      <td>0.674210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36baf228038e314b</td>\n",
       "      <td>0.436187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f37ead645e7f0c8</td>\n",
       "      <td>0.278490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71a5b6ad068d531f</td>\n",
       "      <td>0.037007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     score\n",
       "0  4112d61851461f60  0.462946\n",
       "1  09e418c93a776564  0.674210\n",
       "2  36baf228038e314b  0.436187\n",
       "3  1f37ead645e7f0c8  0.278490\n",
       "4  71a5b6ad068d531f  0.037007"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b189df24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T06:08:32.087581Z",
     "iopub.status.busy": "2022-06-19T06:08:32.087297Z",
     "iopub.status.idle": "2022-06-19T06:08:32.100031Z",
     "shell.execute_reply": "2022-06-19T06:08:32.099030Z"
    },
    "papermill": {
     "duration": 0.283553,
     "end_time": "2022-06-19T06:08:32.102393",
     "exception": false,
     "start_time": "2022-06-19T06:08:31.818840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_1.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1361.833097,
   "end_time": "2022-06-19T06:08:35.587723",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-19T05:45:53.754626",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
